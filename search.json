[
  {
    "objectID": "optimization-gradient-newton.html#最適化問題とは",
    "href": "optimization-gradient-newton.html#最適化問題とは",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "最適化問題とは",
    "text": "最適化問題とは\n機械学習の中心課題\n機械学習では、ほぼすべてのアルゴリズムで最適化問題を解く必要があります：\n\nロジスティック回帰: パラメータを調整して分類精度を最大化\nニューラルネットワーク: 重みを調整して予測誤差を最小化\nサポートベクターマシン: マージンを最大化する境界を見つける\n\n\n\n\n\n\n\n\n機械学習 = 最適化\n\n\n「どうパラメータを調整すれば、最も良い結果が得られるか？」を数値的に解く\n\n\n\n\n今回扱う問題\n次の関数の最小値を求めます：\n\\[f(x) = x^2 - 4x + 3\\]\n目標: \\(f'(x) = 0\\) となる \\(x\\) を見つける\n数学的には簡単ですが、今回は2つの数値的手法で解きます。\n今回の目標\n\n勾配降下法を完全に理解する（手計算 + 実装）\nNewton法を完全に理解する（手計算 + 実装）\n2つの手法を比較して、使い分けを学ぶ"
  },
  {
    "objectID": "optimization-gradient-newton.html#準備pythonのセットアップ",
    "href": "optimization-gradient-newton.html#準備pythonのセットアップ",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "準備：Pythonのセットアップ",
    "text": "準備：Pythonのセットアップ\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 日本語表示設定（必要に応じて）\nplt.rcParams['font.size'] = 11\nplt.rcParams['figure.dpi'] = 100\n\n関数の定義\n\ndef f(x):\n    \"\"\"目的関数 f(x) = x^2 - 4x + 3\"\"\"\n    return x**2 - 4*x + 3\n\ndef f_prime(x):\n    \"\"\"1次微分 f'(x) = 2x - 4\"\"\"\n    return 2*x - 4"
  },
  {
    "objectID": "optimization-gradient-newton.html#勾配降下法とは",
    "href": "optimization-gradient-newton.html#勾配降下法とは",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "勾配降下法とは",
    "text": "勾配降下法とは\n基本的な考え方\n山登りの逆バージョン： 1. 現在地点での傾き（勾配）を測る 2. 傾きの逆方向に少し進む 3. また傾きを測る 4. 繰り返して最小値を探す\nキーワード: 少しずつ、手探りで\nアルゴリズム\n更新式: \\[x_{\\text{new}} = x - \\alpha \\cdot f'(x)\\]\n\n\\(x\\): 現在の位置\n\\(f'(x)\\): 勾配（傾き）\n\\(\\alpha\\): 学習率（どれくらい進むか）\n\\(x_{\\text{new}}\\): 次の位置\n\n\n\n\n\n\n\n警告\n\n\n学習率\\(\\alpha\\)は自分で決める必要がある（これが難しい）"
  },
  {
    "objectID": "optimization-gradient-newton.html#勾配降下法手計算で実行",
    "href": "optimization-gradient-newton.html#勾配降下法手計算で実行",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "勾配降下法：手計算で実行",
    "text": "勾配降下法：手計算で実行\n設定\n\n関数: \\(f(x) = x^2 - 4x + 3\\)\n1次微分: \\(f'(x) = 2x - 4\\)\n初期値: \\(x_0 = 0\\)\n学習率: \\(\\alpha = 0.3\\)"
  },
  {
    "objectID": "optimization-gradient-newton.html#勾配降下法python実装",
    "href": "optimization-gradient-newton.html#勾配降下法python実装",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "勾配降下法：Python実装",
    "text": "勾配降下法：Python実装\n\n\nコードを表示\n# 初期設定\nx = 0.0\nalpha = 0.3  # 学習率\n\nprint(\"勾配降下法で最小値を求める\")\nprint(\"=\" * 60)\n\nfor iteration in range(1, 11):\n    print(f\"\\n--- 反復 {iteration} ---\")\n    print(f\"現在の x = {x:.4f}\")\n    \n    # 関数値\n    fx = f(x)\n    print(f\"f(x) = {fx:.4f}\")\n    \n    # 勾配\n    fp = f_prime(x)\n    print(f\"f'(x) = {fp:.4f}\")\n    \n    # 更新\n    step = alpha * fp\n    x_new = x - step\n    \n    print(f\"\\n更新:\")\n    print(f\"  ステップ幅 = α × f'(x) = {alpha} × {fp:.4f} = {step:.4f}\")\n    print(f\"  x_new = {x:.4f} - {step:.4f} = {x_new:.4f}\")\n    \n    x = x_new\n    \n    # 収束チェック\n    if abs(fp) &lt; 0.01:\n        print(f\"\\n★ ほぼ収束（f'(x) &lt; 0.01）\")\n        break\n\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"最終結果: x = {x:.4f}, f(x) = {f(x):.4f}\")\nprint(f\"理論値との誤差: {abs(x - 2.0):.6f}\")\n\n\n勾配降下法で最小値を求める\n============================================================\n\n--- 反復 1 ---\n現在の x = 0.0000\nf(x) = 3.0000\nf'(x) = -4.0000\n\n更新:\n  ステップ幅 = α × f'(x) = 0.3 × -4.0000 = -1.2000\n  x_new = 0.0000 - -1.2000 = 1.2000\n\n--- 反復 2 ---\n現在の x = 1.2000\nf(x) = -0.3600\nf'(x) = -1.6000\n\n更新:\n  ステップ幅 = α × f'(x) = 0.3 × -1.6000 = -0.4800\n  x_new = 1.2000 - -0.4800 = 1.6800\n\n--- 反復 3 ---\n現在の x = 1.6800\nf(x) = -0.8976\nf'(x) = -0.6400\n\n更新:\n  ステップ幅 = α × f'(x) = 0.3 × -0.6400 = -0.1920\n  x_new = 1.6800 - -0.1920 = 1.8720\n\n--- 反復 4 ---\n現在の x = 1.8720\nf(x) = -0.9836\nf'(x) = -0.2560\n\n更新:\n  ステップ幅 = α × f'(x) = 0.3 × -0.2560 = -0.0768\n  x_new = 1.8720 - -0.0768 = 1.9488\n\n--- 反復 5 ---\n現在の x = 1.9488\nf(x) = -0.9974\nf'(x) = -0.1024\n\n更新:\n  ステップ幅 = α × f'(x) = 0.3 × -0.1024 = -0.0307\n  x_new = 1.9488 - -0.0307 = 1.9795\n\n--- 反復 6 ---\n現在の x = 1.9795\nf(x) = -0.9996\nf'(x) = -0.0410\n\n更新:\n  ステップ幅 = α × f'(x) = 0.3 × -0.0410 = -0.0123\n  x_new = 1.9795 - -0.0123 = 1.9918\n\n--- 反復 7 ---\n現在の x = 1.9918\nf(x) = -0.9999\nf'(x) = -0.0164\n\n更新:\n  ステップ幅 = α × f'(x) = 0.3 × -0.0164 = -0.0049\n  x_new = 1.9918 - -0.0049 = 1.9967\n\n--- 反復 8 ---\n現在の x = 1.9967\nf(x) = -1.0000\nf'(x) = -0.0066\n\n更新:\n  ステップ幅 = α × f'(x) = 0.3 × -0.0066 = -0.0020\n  x_new = 1.9967 - -0.0020 = 1.9987\n\n★ ほぼ収束（f'(x) &lt; 0.01）\n\n============================================================\n最終結果: x = 1.9987, f(x) = -1.0000\n理論値との誤差: 0.001311"
  },
  {
    "objectID": "optimization-gradient-newton.html#勾配降下法の問題点",
    "href": "optimization-gradient-newton.html#勾配降下法の問題点",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "勾配降下法の問題点",
    "text": "勾配降下法の問題点\n1. 学習率の選択が難しい\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n警告\n\n\n\n\\(\\alpha\\)が小さすぎる: 収束が遅い\n\\(\\alpha\\)が大きすぎる: 振動して収束しない\n適切な\\(\\alpha\\)を探すのが難しい\n\n\n\n\n2. 収束が遅い\n10回反復してもまだ誤差がある。複雑な問題では数百〜数千回必要になることも。"
  },
  {
    "objectID": "optimization-gradient-newton.html#なぜnewton法が必要か",
    "href": "optimization-gradient-newton.html#なぜnewton法が必要か",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "なぜNewton法が必要か",
    "text": "なぜNewton法が必要か\n勾配降下法の問題： - 学習率を手動で調整 - 収束が遅い\n解決策: もっと賢く進めないか？\n\n\n\n\n\n\n\nNewton法のアイデア\n\n\n「傾き」だけでなく「曲がり具合」も使えば、どこまで進めばいいか計算できるのでは？"
  },
  {
    "objectID": "optimization-gradient-newton.html#次微分の役割",
    "href": "optimization-gradient-newton.html#次微分の役割",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "2次微分の役割",
    "text": "2次微分の役割\n2次微分とは\n1次微分: 傾き（どっちに進むか） \\[f'(x) = 2x - 4\\]\n2次微分: 曲率（どれくらい曲がっているか） \\[f''(x) = \\frac{d}{dx}(2x - 4) = 2\\]\n\n\n\n\n\n\n\n2次微分の計算\n\n\n1次微分をもう一度微分するだけ： \\[f'(x) = 2x - 4\\] \\[f''(x) = 2 \\times 1 - 0 = 2\\]\n\n\n\n\nヘッセ行列（Hessian Matrix）とは\n注意: 1変数の場合、2次微分は単なる数値なので「ヘッセ行列」とは呼びません。\nヘッセ行列は2変数以上で使う用語です：\n2変数の場合（次回学びます）: \\[H = \\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\\n\\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2}\n\\end{bmatrix}\\]\n\n\n\n\n\n\nノート\n\n\n\n1変数: \\(f''(x)\\) = 2次微分（スカラー）\n2変数以上: \\(H\\) = ヘッセ行列（行列）\n\n今回は1変数なので「2次微分」と呼びます。"
  },
  {
    "objectID": "optimization-gradient-newton.html#newton法のアルゴリズム",
    "href": "optimization-gradient-newton.html#newton法のアルゴリズム",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "Newton法のアルゴリズム",
    "text": "Newton法のアルゴリズム\n更新式\n\\[x_{\\text{new}} = x - \\frac{f'(x)}{f''(x)}\\]\n\n\\(f'(x)\\): 1次微分（勾配）\n\\(f''(x)\\): 2次微分（曲率）\n学習率不要！\n\nなぜこの式？\n現在地点で関数を2次関数で近似： \\[f(x) \\approx f(x_0) + f'(x_0)(x-x_0) + \\frac{1}{2}f''(x_0)(x-x_0)^2\\]\nこの2次関数の最小値を求めると： \\[x_{\\text{min}} = x_0 - \\frac{f'(x_0)}{f''(x_0)}\\]\n\n\n\n\n\n\n重要\n\n\n2次関数の最小値は解析的に求められる → 一気にそこまでジャンプできる！"
  },
  {
    "objectID": "optimization-gradient-newton.html#newton法手計算で実行",
    "href": "optimization-gradient-newton.html#newton法手計算で実行",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "Newton法：手計算で実行",
    "text": "Newton法：手計算で実行\n設定\n\n関数: \\(f(x) = x^2 - 4x + 3\\)\n1次微分: \\(f'(x) = 2x - 4\\)\n2次微分: \\(f''(x) = 2\\)（重要！）\n初期値: \\(x_0 = 0\\)"
  },
  {
    "objectID": "optimization-gradient-newton.html#newton法python実装",
    "href": "optimization-gradient-newton.html#newton法python実装",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "Newton法：Python実装",
    "text": "Newton法：Python実装\n2次微分の定義\n\ndef f_double_prime(x):\n    \"\"\"2次微分 f''(x) = 2\"\"\"\n    return 2\n\n\n\n\n\n\n\n\n手で微分して関数定義\n\n\n\n\\(f(x) = x^2 - 4x + 3\\)\n\\(f'(x) = 2x - 4\\)\n\\(f''(x) = 2\\)\n\n手で計算した結果を関数として定義します。 自動微分は使いません。\n\n\n\n\nNewton法の実装\n\n\nコードを表示\n# 初期値\nx = 0.0\n\nprint(\"Newton法で最小値を求める\")\nprint(\"=\" * 60)\n\nfor iteration in range(1, 6):\n    print(f\"\\n--- 反復 {iteration} ---\")\n    print(f\"現在の x = {x:.4f}\")\n    \n    # 関数値\n    fx = f(x)\n    print(f\"f(x) = {fx:.4f}\")\n    \n    # 1次微分（勾配）\n    fp = f_prime(x)\n    print(f\"f'(x) = {fp:.4f}\")\n    \n    # 2次微分（曲率）\n    fpp = f_double_prime(x)\n    print(f\"f''(x) = {fpp:.4f}\")\n    \n    # Newton法の更新\n    step = fp / fpp\n    x_new = x - step\n    \n    print(f\"\\n更新:\")\n    print(f\"  ステップ幅 = f'(x)/f''(x) = {fp:.4f}/{fpp:.4f} = {step:.4f}\")\n    print(f\"  x_new = {x:.4f} - {step:.4f} = {x_new:.4f}\")\n    \n    x = x_new\n    \n    # 収束チェック\n    if abs(fp) &lt; 0.0001:\n        print(f\"\\n★ 収束！ f'(x) ≈ 0\")\n        break\n\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"最終結果: x = {x:.4f}, f(x) = {f(x):.4f}\")\n\n\nNewton法で最小値を求める\n============================================================\n\n--- 反復 1 ---\n現在の x = 0.0000\nf(x) = 3.0000\nf'(x) = -4.0000\nf''(x) = 2.0000\n\n更新:\n  ステップ幅 = f'(x)/f''(x) = -4.0000/2.0000 = -2.0000\n  x_new = 0.0000 - -2.0000 = 2.0000\n\n--- 反復 2 ---\n現在の x = 2.0000\nf(x) = -1.0000\nf'(x) = 0.0000\nf''(x) = 2.0000\n\n更新:\n  ステップ幅 = f'(x)/f''(x) = 0.0000/2.0000 = 0.0000\n  x_new = 2.0000 - 0.0000 = 2.0000\n\n★ 収束！ f'(x) ≈ 0\n\n============================================================\n最終結果: x = 2.0000, f(x) = -1.0000"
  },
  {
    "objectID": "optimization-gradient-newton.html#可視化で比較",
    "href": "optimization-gradient-newton.html#可視化で比較",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "可視化で比較",
    "text": "可視化で比較"
  },
  {
    "objectID": "optimization-gradient-newton.html#比較まとめ",
    "href": "optimization-gradient-newton.html#比較まとめ",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "比較まとめ",
    "text": "比較まとめ\n数値比較\n\n\n\n手法\n反復回数\n最終結果\n誤差\n\n\n\n\n勾配降下法（α=0.3）\n10回\nx=1.9926\n0.0074\n\n\nNewton法\n1回\nx=2.0000\n0.0000\n\n\n\n特徴比較\n\n\n\n項目\n勾配降下法\nNewton法\n\n\n\n\n使う情報\n1次微分のみ\n1次微分 + 2次微分\n\n\n学習率\n必要（α）\n不要\n\n\n収束速度\n遅い\n非常に速い\n\n\n1回の計算量\n小\n大\n\n\nパラメータ調整\n必要\n不要\n\n\n適用場面\n大規模問題\n小〜中規模問題\n\n\n実例\nPyTorch、TensorFlow\nscikit-learn"
  },
  {
    "objectID": "optimization-gradient-newton.html#なぜnewton法は速いのか",
    "href": "optimization-gradient-newton.html#なぜnewton法は速いのか",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "なぜNewton法は速いのか",
    "text": "なぜNewton法は速いのか\n視覚的説明\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2次近似の威力\n\n\n勾配降下法: 1次近似（接線）だけ → 「どっちに進むか」はわかるが「どこまで進むか」は手探り\nNewton法: 2次近似（放物線） → 放物線の最小値を計算 → 一気にジャンプ"
  },
  {
    "objectID": "optimization-gradient-newton.html#使い分けの指針",
    "href": "optimization-gradient-newton.html#使い分けの指針",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "使い分けの指針",
    "text": "使い分けの指針\n\n\n\n\n\n\nどちらを使うべきか\n\n\n勾配降下法を使う場合: - パラメータ数が多い（数万〜数百万） - メモリが限られている - オンライン学習（データが逐次的に来る） - 例：ディープラーニング\nNewton法を使う場合: - パラメータ数が少ない（数個〜数千） - 高精度な解が必要 - バッチ学習 - 例：ロジスティック回帰、一般化線形モデル"
  },
  {
    "objectID": "optimization-gradient-newton.html#まとめ",
    "href": "optimization-gradient-newton.html#まとめ",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "まとめ",
    "text": "まとめ\n勾配降下法\n長所: - シンプルで実装が簡単 - 大規模問題に対応可能 - メモリ効率が良い\n短所: - 学習率の調整が必要 - 収束が遅い\nNewton法\n長所: - 学習率不要（自動計算） - 収束が非常に速い - 高精度\n短所: - 2次微分（ヘッセ行列）の計算が必要 - 大規模問題には不向き\n\n\n\n\n\n\nノート\n\n\nどちらも重要！ 問題の性質に応じて使い分けることが大切です。"
  },
  {
    "objectID": "optimization-gradient-newton.html#次回予告",
    "href": "optimization-gradient-newton.html#次回予告",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "次回予告",
    "text": "次回予告\n次回は2変数のNewton法を学びます：\n\nヘッセ行列（2×2行列）の計算\n逆行列の手計算\nロジスティック回帰への応用\n実際のデータでの分類問題"
  },
  {
    "objectID": "optimization-gradient-newton.html#練習問題",
    "href": "optimization-gradient-newton.html#練習問題",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "練習問題",
    "text": "練習問題\n以下の問題を手計算とPythonコードの両方で解け。\n問題1：勾配降下法\n関数 \\(f(x) = x^2 + 6x + 5\\) について：\n\n1次微分を求めよ\n初期値 \\(x_0 = 0\\)、学習率 \\(\\alpha = 0.2\\) で勾配降下法を実行せよ\n反復1、反復2、反復3を手計算で示せ\n\n問題2：Newton法\n同じ関数 \\(f(x) = x^2 + 6x + 5\\) について：\n\n2次微分を求めよ\n初期値 \\(x_0 = 0\\) でNewton法を実行せよ\n反復1、反復2を手計算で示せ\n\n問題3：比較\n\n問題1と問題2の結果を比較せよ\n何回の反復で収束したか？\nどちらが速かったか？"
  },
  {
    "objectID": "optimization-gradient-newton.html#問題1の解答例",
    "href": "optimization-gradient-newton.html#問題1の解答例",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "問題1の解答例",
    "text": "問題1の解答例\n準備\n\\[f(x) = x^2 + 6x + 5\\]\n1次微分: \\[f'(x) = 2x + 6\\]\n反復1\n現在: \\(x = 0\\)\n\\[f'(0) = 2(0) + 6 = 6\\]\n更新: \\[x_{\\text{new}} = 0 - 0.2 \\times 6 = 0 - 1.2 = -1.2\\]\n反復2\n現在: \\(x = -1.2\\)\n\\[f'(-1.2) = 2(-1.2) + 6 = -2.4 + 6 = 3.6\\]\n更新: \\[x_{\\text{new}} = -1.2 - 0.2 \\times 3.6 = -1.2 - 0.72 = -1.92\\]\n反復3\n現在: \\(x = -1.92\\)\n\\[f'(-1.92) = 2(-1.92) + 6 = -3.84 + 6 = 2.16\\]\n更新: \\[x_{\\text{new}} = -1.92 - 0.2 \\times 2.16 = -1.92 - 0.432 = -2.352\\]\nまだ収束していない（理論値は\\(x=-3\\)）"
  },
  {
    "objectID": "optimization-gradient-newton.html#参考文献",
    "href": "optimization-gradient-newton.html#参考文献",
    "title": "最適化問題入門：勾配降下法とNewton法",
    "section": "参考文献",
    "text": "参考文献\n教科書\n\nStephen Boyd & Lieven Vandenberghe, “Convex Optimization” (2004)\nJorge Nocedal & Stephen Wright, “Numerical Optimization” (2006)\n\n日本語資料\n\n杉山将「イラストで学ぶ機械学習」(2013)\n金谷健一「これなら分かる最適化数学」(2005)\n\nオンライン資料\n\nscipy.optimize documentation\nscikit-learn: Logistic Regression"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#前回やったこと",
    "href": "optimization-local-minima-appendix.html#前回やったこと",
    "title": "ローカルミニマムと初期値",
    "section": "前回やったこと",
    "text": "前回やったこと\n前回は \\(f(x) = x^2 - 4x + 3\\) という2次関数で、勾配降下法とNewton法を学びました。\nこの関数は谷が1つだけのシンプルな形でした。 どこから始めても、必ず同じ最小値 \\(x = 2\\) にたどり着きました。\n\n\n\n\n\n\n前回の結論\n\n\n\n勾配降下法：何回も反復して少しずつ最小値に近づく\nNewton法：2次微分も使って、一気に最小値にジャンプ\nどちらも同じ答えにたどり着いた"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#今回の疑問",
    "href": "optimization-local-minima-appendix.html#今回の疑問",
    "title": "ローカルミニマムと初期値",
    "section": "今回の疑問",
    "text": "今回の疑問\nでも、こんな疑問が浮かびます：\n\n「谷が2つ以上ある関数だったら、どうなるの？」\n\n実際の機械学習の問題では、谷が1つだけということはほとんどありません。 パラメータが多くなると、たくさんの谷が存在します。\n今回は「谷が2つある関数」で、Newton法を使ったときに何が起こるかを体験しましょう。"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#関数の定義",
    "href": "optimization-local-minima-appendix.html#関数の定義",
    "title": "ローカルミニマムと初期値",
    "section": "関数の定義",
    "text": "関数の定義\n今回使う関数はこれです：\n\\[f(x) = 0.1x^4 - x^3 + 2x^2 + 1\\]\n前回と同じように、微分を手で計算しておきます。\n1次微分（傾き）: \\[f'(x) = 0.4x^3 - 3x^2 + 4x\\]\n2次微分（曲がり具合）: \\[f''(x) = 1.2x^2 - 6x + 4\\]"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#pythonでの定義",
    "href": "optimization-local-minima-appendix.html#pythonでの定義",
    "title": "ローカルミニマムと初期値",
    "section": "Pythonでの定義",
    "text": "Pythonでの定義\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 4次関数とその微分\ndef f_poly(x):\n    return 0.1*x**4 - x**3 + 2*x**2 + 1\n\ndef f_poly_prime(x):\n    return 0.4*x**3 - 3*x**2 + 4*x\n\ndef f_poly_double_prime(x):\n    return 1.2*x**2 - 6*x + 4"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#まずグラフを描いてみよう",
    "href": "optimization-local-minima-appendix.html#まずグラフを描いてみよう",
    "title": "ローカルミニマムと初期値",
    "section": "まずグラフを描いてみよう",
    "text": "まずグラフを描いてみよう\nどんな形の関数なのか、まず目で見ることが大切です。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nグラフを観察しよう\n\n\n谷がいくつあるか、数えてみてください。\n\n左の谷（\\(x \\approx 1\\) 付近）：浅い谷\n右の谷（\\(x \\approx 6\\) 付近）：深い谷\n\n前回の2次関数は谷が1つだけでしたが、今回は2つあります。\nさて、Newton法を使ったら、どちらの谷に行くのでしょうか？"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#その前にfx-0-は谷だけじゃない",
    "href": "optimization-local-minima-appendix.html#その前にfx-0-は谷だけじゃない",
    "title": "ローカルミニマムと初期値",
    "section": "その前に：\\(f'(x) = 0\\) は「谷」だけじゃない",
    "text": "その前に：\\(f'(x) = 0\\) は「谷」だけじゃない\n実験に入る前に、1つだけ知っておくべきことがあります。\nNewton法は \\(f'(x) = 0\\) となる点を探しますが、\\(f'(x) = 0\\) になるのは谷（最小値）だけではありません。山（最大値）でも \\(f'(x) = 0\\) です。\nこれは2次微分で見分けられます：\n\\[f''(x) &gt; 0 \\Rightarrow \\text{谷（最小値）}\\] \\[f''(x) &lt; 0 \\Rightarrow \\text{山（最大値）}\\]\n\n\n\n\n\n\n覚えておこう\n\n\n収束したら \\(f''(x)\\) をチェック。正なら谷、負なら山。 山に止まった結果は捨てましょう。"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#実験1x_0-1.5-からスタート左の谷の近く",
    "href": "optimization-local-minima-appendix.html#実験1x_0-1.5-からスタート左の谷の近く",
    "title": "ローカルミニマムと初期値",
    "section": "実験1：\\(x_0 = 1.5\\) からスタート（左の谷の近く）",
    "text": "実験1：\\(x_0 = 1.5\\) からスタート（左の谷の近く）\n手計算で追いかけてみよう\n反復1\n現在の位置: \\(x = 1.5\\)\n1次微分（傾き）を計算: \\[f'(1.5) = 0.4 \\times 1.5^3 - 3 \\times 1.5^2 + 4 \\times 1.5\\] \\[= 0.4 \\times 3.375 - 3 \\times 2.25 + 6.0\\] \\[= 1.35 - 6.75 + 6.0 = 0.6\\]\n2次微分（曲がり具合）を計算: \\[f''(1.5) = 1.2 \\times 1.5^2 - 6 \\times 1.5 + 4\\] \\[= 1.2 \\times 2.25 - 9.0 + 4.0\\] \\[= 2.7 - 9.0 + 4.0 = -2.3\\]\n更新: \\[x_{\\text{new}} = 1.5 - \\frac{0.6}{-2.3} = 1.5 - (-0.261) = 1.5 + 0.261 = 1.761\\]\n反復2\n現在の位置: \\(x = 1.761\\)\n1次微分: \\[f'(1.761) = 0.4 \\times 1.761^3 - 3 \\times 1.761^2 + 4 \\times 1.761\\] \\[= 2.184 - 9.302 + 7.044 = -0.074\\]\n2次微分: \\[f''(1.761) = 1.2 \\times 1.761^2 - 6 \\times 1.761 + 4\\] \\[= 3.720 - 10.566 + 4.0 = -2.846\\]\n更新: \\[x_{\\text{new}} = 1.761 - \\frac{-0.074}{-2.846} = 1.761 - 0.026 = 1.735\\]\n反復3\n\\(f'(1.735)\\) はほぼゼロに近づいています。収束しました！\n\n\n\n\n\n\n実験1の結果\n\n\n\\(x_0 = 1.5\\) からスタート → \\(x \\approx 1.74\\) に収束\n\\(f(1.74) \\approx 1.93\\)\n\n\n\nPythonで確認\n\n\nコードを表示\n# 実験1: x0 = 1.5 からスタート\nx0_local = 1.5\nx = x0_local\nhistory_local = [x]\n\nprint(\"実験1: x0 = 1.5 からスタート\")\nprint(\"=\" * 65)\n\nfor iteration in range(1, 8):\n    fp = f_poly_prime(x)\n    fpp = f_poly_double_prime(x)\n    \n    print(f\"反復 {iteration}: x = {x:.4f}, f(x) = {f_poly(x):.4f}, f'(x) = {fp:.4f}, f''(x) = {fpp:.4f}\")\n    \n    if abs(fp) &lt; 0.001:\n        print(f\"\\n★ 収束！ x = {x:.4f}, f(x) = {f_poly(x):.4f}\")\n        break\n    \n    if abs(fpp) &lt; 1e-10:\n        print(f\"\\n2次微分が小さすぎる\")\n        break\n    \n    x_new = x - fp / fpp\n    print(f\"  更新: x_new = {x:.4f} - ({fp:.4f})/({fpp:.4f}) = {x_new:.4f}\")\n    history_local.append(x_new)\n    x = x_new\n\nx_local_final = x\nf_local_final = f_poly(x)\n\n\n実験1: x0 = 1.5 からスタート\n=================================================================\n反復 1: x = 1.5000, f(x) = 2.6313, f'(x) = 0.6000, f''(x) = -2.3000\n  更新: x_new = 1.5000 - (0.6000)/(-2.3000) = 1.7609\n反復 2: x = 1.7609, f(x) = 2.7029, f'(x) = -0.0746, f''(x) = -2.8444\n  更新: x_new = 1.7609 - (-0.0746)/(-2.8444) = 1.7347\n反復 3: x = 1.7347, f(x) = 2.7039, f'(x) = -0.0006, f''(x) = -2.7971\n\n★ 収束！ x = 1.7347, f(x) = 2.7039"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#実験2x_0-5.5-からスタート右の谷の近く",
    "href": "optimization-local-minima-appendix.html#実験2x_0-5.5-からスタート右の谷の近く",
    "title": "ローカルミニマムと初期値",
    "section": "実験2：\\(x_0 = 5.5\\) からスタート（右の谷の近く）",
    "text": "実験2：\\(x_0 = 5.5\\) からスタート（右の谷の近く）\n手計算で追いかけてみよう\n反復1\n現在の位置: \\(x = 5.5\\)\n1次微分: \\[f'(5.5) = 0.4 \\times 5.5^3 - 3 \\times 5.5^2 + 4 \\times 5.5\\] \\[= 0.4 \\times 166.375 - 3 \\times 30.25 + 22.0\\] \\[= 66.55 - 90.75 + 22.0 = -2.2\\]\n2次微分: \\[f''(5.5) = 1.2 \\times 5.5^2 - 6 \\times 5.5 + 4\\] \\[= 1.2 \\times 30.25 - 33.0 + 4.0\\] \\[= 36.3 - 33.0 + 4.0 = 7.3\\]\n更新: \\[x_{\\text{new}} = 5.5 - \\frac{-2.2}{7.3} = 5.5 + 0.301 = 5.801\\]\n反復2\n現在の位置: \\(x = 5.801\\)\n1次微分: \\[f'(5.801) = 0.4 \\times 5.801^3 - 3 \\times 5.801^2 + 4 \\times 5.801\\] \\[= 78.11 - 100.95 + 23.20 = 0.36\\]\n2次微分: \\[f''(5.801) = 1.2 \\times 5.801^2 - 6 \\times 5.801 + 4\\] \\[= 40.37 - 34.81 + 4.0 = 9.56\\]\n更新: \\[x_{\\text{new}} = 5.801 - \\frac{0.36}{9.56} = 5.801 - 0.038 = 5.763\\]\n反復3\n\\(f'(5.763)\\) はほぼゼロ。収束しました！\n\n\n\n\n\n\n実験2の結果\n\n\n\\(x_0 = 5.5\\) からスタート → \\(x \\approx 5.76\\) に収束\n\\(f(5.76) \\approx -3.04\\)\n\n\n\nPythonで確認\n\n\nコードを表示\n# 実験2: x0 = 5.5 からスタート\nx0_global = 5.5\nx = x0_global\nhistory_global = [x]\n\nprint(\"実験2: x0 = 5.5 からスタート\")\nprint(\"=\" * 65)\n\nfor iteration in range(1, 8):\n    fp = f_poly_prime(x)\n    fpp = f_poly_double_prime(x)\n    \n    print(f\"反復 {iteration}: x = {x:.4f}, f(x) = {f_poly(x):.4f}, f'(x) = {fp:.4f}, f''(x) = {fpp:.4f}\")\n    \n    if abs(fp) &lt; 0.001:\n        print(f\"\\n★ 収束！ x = {x:.4f}, f(x) = {f_poly(x):.4f}\")\n        break\n    \n    if abs(fpp) &lt; 1e-10:\n        print(f\"\\n2次微分が小さすぎる\")\n        break\n    \n    x_new = x - fp / fpp\n    print(f\"  更新: x_new = {x:.4f} - ({fp:.4f})/({fpp:.4f}) = {x_new:.4f}\")\n    history_global.append(x_new)\n    x = x_new\n\nx_global_final = x\nf_global_final = f_poly(x)\n\n\n実験2: x0 = 5.5 からスタート\n=================================================================\n反復 1: x = 5.5000, f(x) = -13.3687, f'(x) = -2.2000, f''(x) = 7.3000\n  更新: x_new = 5.5000 - (-2.2000)/(7.3000) = 5.8014\n反復 2: x = 5.8014, f(x) = -13.6666, f'(x) = 0.3379, f''(x) = 9.5789\n  更新: x_new = 5.8014 - (0.3379)/(9.5789) = 5.7661\n反復 3: x = 5.7661, f(x) = -13.6726, f'(x) = 0.0049, f''(x) = 9.3008\n  更新: x_new = 5.7661 - (0.0049)/(9.3008) = 5.7656\n反復 4: x = 5.7656, f(x) = -13.6726, f'(x) = 0.0000, f''(x) = 9.2967\n\n★ 収束！ x = 5.7656, f(x) = -13.6726"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#つの実験を並べてみよう",
    "href": "optimization-local-minima-appendix.html#つの実験を並べてみよう",
    "title": "ローカルミニマムと初期値",
    "section": "2つの実験を並べてみよう",
    "text": "2つの実験を並べてみよう\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n答えが違う\n\n\nまったく同じNewton法を使ったのに、結果が違う：\n\n\n\n\nスタート地点\n到着地点\nf(x)の値\n\n\n\n\n実験1\n\\(x_0 = 1.5\\)\n\\(x \\approx\\) 1.73\n2.70\n\n\n実験2\n\\(x_0 = 5.5\\)\n\\(x \\approx\\) 5.77\n-13.67\n\n\n\n実験2のほうが \\(f(x)\\) が小さい = より良い答えを見つけている！\nスタート地点（初期値）が違うだけで、たどり着く場所が変わってしまう。"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#ローカルミニマムとグローバルミニマム",
    "href": "optimization-local-minima-appendix.html#ローカルミニマムとグローバルミニマム",
    "title": "ローカルミニマムと初期値",
    "section": "ローカルミニマムとグローバルミニマム",
    "text": "ローカルミニマムとグローバルミニマム\nいま体験した現象には、ちゃんと名前がついています。\nローカルミニマム（局所的最小値）: 「その周辺では一番低い」けど、関数全体で見るともっと低い場所がある点。 実験1で到達した左の谷がこれです。\nグローバルミニマム（大域的最小値）: 関数全体で本当に一番低い点。 実験2で到達した右の谷がこれです。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nたとえ話：山の中で一番低い場所を探す\n\n\nあなたは目隠しをして山の中にいます。足元の傾きだけを頼りに、一番低い場所を探します。\n\n近くの谷に降りることはできる → ローカルミニマム\nでも、山の向こうにもっと深い谷があるかもしれない → グローバルミニマム\n目隠しをしているので、今いる谷が一番深いかどうかわからない！\n\nNewton法も勾配降下法も、この「目隠し」と同じ状況です。"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#newton法は近くの谷に落ちる",
    "href": "optimization-local-minima-appendix.html#newton法は近くの谷に落ちる",
    "title": "ローカルミニマムと初期値",
    "section": "Newton法は「近くの谷」に落ちる",
    "text": "Newton法は「近くの谷」に落ちる\nNewton法は、現在地の近くにある谷を見つけるアルゴリズムです。 遠くの谷のことは知りません。\nだから：\n\n左の谷の近くからスタート → 左の谷（ローカルミニマム）に落ちる\n右の谷の近くからスタート → 右の谷（グローバルミニマム）に落ちる"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#引力圏basin-of-attractionを可視化しよう",
    "href": "optimization-local-minima-appendix.html#引力圏basin-of-attractionを可視化しよう",
    "title": "ローカルミニマムと初期値",
    "section": "引力圏（Basin of Attraction）を可視化しよう",
    "text": "引力圏（Basin of Attraction）を可視化しよう\n「どの初期値からスタートすると、どの谷に落ちるか」を全部調べてみましょう。\n\n\nコードを表示\ndef newton_optimize(f, fp, fpp, x0, max_iter=50):\n    \"\"\"Newton法を実行して結果を返す\"\"\"\n    x = x0\n    for i in range(max_iter):\n        grad = fp(x)\n        hess = fpp(x)\n        if abs(grad) &lt; 1e-6:\n            return x, f(x), True\n        if abs(hess) &lt; 1e-10:\n            return x, f(x), False\n        x = x - grad / hess\n    return x, f(x), False\n\n# 初期値を -1.5 から 7.5 まで細かく変えて、全部試す\nx0_dense = np.linspace(-1.5, 7.5, 200)\nconvergence_points = []\nconvergence_values = []\n\nfor x0 in x0_dense:\n    x_final, f_final, _ = newton_optimize(f_poly, f_poly_prime, f_poly_double_prime, x0)\n    convergence_points.append(x_final)\n    convergence_values.append(f_final)\n\nconvergence_points = np.array(convergence_points)\nconvergence_values = np.array(convergence_values)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n引力圏（Basin of Attraction）とは\n\n\n各谷には「引力圏」があります。その範囲内からスタートすると、その谷に吸い込まれるという領域です。\n\n赤い領域からスタート → ローカルミニマム（浅い谷）に落ちる\n緑の領域からスタート → グローバルミニマム（深い谷）に落ちる\n\n初期値が少し変わるだけで、結果がガラッと変わる境界があることに注目してください。"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#考え方1回だけでは信用できない",
    "href": "optimization-local-minima-appendix.html#考え方1回だけでは信用できない",
    "title": "ローカルミニマムと初期値",
    "section": "考え方：1回だけでは信用できない",
    "text": "考え方：1回だけでは信用できない\nここまでの実験でわかったこと：\n\nNewton法は「近くの谷」に落ちるだけ\n1回の実行では、それがローカルかグローバルかわからない\n\n解決策はシンプル：何回もやって、一番良い結果を選ぶ！"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#方法1手で初期値を変えて比較する",
    "href": "optimization-local-minima-appendix.html#方法1手で初期値を変えて比較する",
    "title": "ローカルミニマムと初期値",
    "section": "方法1：手で初期値を変えて比較する",
    "text": "方法1：手で初期値を変えて比較する\nまず、3つの初期値から手計算で比較してみましょう。\n\\(x_0 = 0\\) からスタート\n\\[f'(0) = 0.4 \\times 0 - 3 \\times 0 + 4 \\times 0 = 0\\]\n\\(f'(0) = 0\\) なので、\\(x = 0\\) は既に極値です。\\(f(0) = 1\\)。\n\\(f''(0) = 1.2 \\times 0 - 6 \\times 0 + 4 = 4 &gt; 0\\) → 最小値（谷）。\n\\(x_0 = 2\\) からスタート\n\\[f'(2) = 0.4 \\times 8 - 3 \\times 4 + 4 \\times 2 = 3.2 - 12 + 8 = -0.8\\] \\[f''(2) = 1.2 \\times 4 - 6 \\times 2 + 4 = 4.8 - 12 + 4 = -3.2\\] \\[x_{\\text{new}} = 2 - \\frac{-0.8}{-3.2} = 2 - 0.25 = 1.75\\]\n→ 左の谷（ローカルミニマム）の方向に進んでいます。\n\\(x_0 = 6\\) からスタート\n\\[f'(6) = 0.4 \\times 216 - 3 \\times 36 + 4 \\times 6 = 86.4 - 108 + 24 = 2.4\\] \\[f''(6) = 1.2 \\times 36 - 6 \\times 6 + 4 = 43.2 - 36 + 4 = 11.2\\] \\[x_{\\text{new}} = 6 - \\frac{2.4}{11.2} = 6 - 0.214 = 5.786\\]\n→ 右の谷（グローバルミニマム）の方向に進んでいます。\n3つの結果を比較\n\n# 3つの初期値から実行して比較\nstarting_points = [0, 2, 6]\n\nprint(\"初期値を変えて比較\")\nprint(\"=\" * 55)\nprint(f\"{'x0':&gt;4}  →  {'到着地点':&gt;8}  →  {'f(x)':&gt;8}  →  判定\")\nprint(\"-\" * 55)\n\nresults_comparison = []\nfor x0 in starting_points:\n    x_final, f_final, _ = newton_optimize(f_poly, f_poly_prime, f_poly_double_prime, x0)\n    results_comparison.append((x0, x_final, f_final))\n\nbest_f = min(r[2] for r in results_comparison)\nfor x0, xf, fx in results_comparison:\n    marker = \" ← 最良！\" if fx == best_f else \"\"\n    print(f\"{x0:4.1f}  →  {xf:8.4f}  →  {fx:8.4f}  →{marker}\")\n\n初期値を変えて比較\n=======================================================\n  x0  →      到着地点  →      f(x)  →  判定\n-------------------------------------------------------\n 0.0  →    0.0000  →    1.0000  →\n 2.0  →    1.7344  →    2.7039  →\n 6.0  →    5.7656  →  -13.6726  → ← 最良！\n\n\n\n\n\n\n\n\nポイント\n\n\n3つの初期値を試すだけで、一番良い答えを選べるようになりました！\n手間は3倍になりますが、間違った答えを信じるよりずっと良いです。"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#方法2たくさんの初期値を自動で試すランダムリスタート",
    "href": "optimization-local-minima-appendix.html#方法2たくさんの初期値を自動で試すランダムリスタート",
    "title": "ローカルミニマムと初期値",
    "section": "方法2：たくさんの初期値を自動で試す（ランダムリスタート）",
    "text": "方法2：たくさんの初期値を自動で試す（ランダムリスタート）\n手で3つ試すのも良いですが、コンピュータなら20個でも100個でも試せます。\n\n\nコードを表示\n# 20個のランダムな初期値から試す\nnp.random.seed(42)\nn_trials = 20\nresults = []\n\nprint(\"ランダムリスタート：20個の初期値から試す\")\nprint(\"=\" * 60)\n\nfor i in range(n_trials):\n    x0 = np.random.uniform(-1, 7)\n    x_final, f_final, success = newton_optimize(f_poly, f_poly_prime, f_poly_double_prime, x0)\n    results.append((x0, x_final, f_final, success))\n    \n    if i &lt; 5:\n        print(f\"試行 {i+1:2d}: x0={x0:5.2f} → x={x_final:6.2f}, f(x)={f_final:7.4f}\")\n\nprint(f\"  ... (残り{n_trials-5}個)\")\n\n# 最良の結果を見つける\nresults.sort(key=lambda r: r[2])\nbest_x0, best_x, best_f, _ = results[0]\n\nprint(f\"\\n★ 最良の結果: x0={best_x0:.2f} → x={best_x:.4f}, f(x)={best_f:.4f}\")\n\n\nランダムリスタート：20個の初期値から試す\n============================================================\n試行  1: x0= 2.00 → x=  1.73, f(x)= 2.7039\n試行  2: x0= 6.61 → x=  5.77, f(x)=-13.6726\n試行  3: x0= 4.86 → x=  5.77, f(x)=-13.6726\n試行  4: x0= 3.79 → x= -0.00, f(x)= 1.0000\n試行  5: x0= 0.25 → x= -0.00, f(x)= 1.0000\n  ... (残り15個)\n\n★ 最良の結果: x0=5.93 → x=5.7656, f(x)=-13.6726"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#方法3等間隔に初期値を並べるグリッドサーチ",
    "href": "optimization-local-minima-appendix.html#方法3等間隔に初期値を並べるグリッドサーチ",
    "title": "ローカルミニマムと初期値",
    "section": "方法3：等間隔に初期値を並べる（グリッドサーチ）",
    "text": "方法3：等間隔に初期値を並べる（グリッドサーチ）\nランダムではなく、等間隔に並べる方法もあります。\n\n\nコードを表示\n# 等間隔に15個の初期値を用意\nx0_candidates = np.linspace(-1, 7, 15)\ngrid_results = []\n\nprint(\"グリッドサーチ：等間隔の15個の初期値\")\nprint(\"=\" * 55)\nprint(f\"{'x0':&gt;5}  →  {'到着地点':&gt;8}  →  {'f(x)':&gt;8}\")\nprint(\"-\" * 55)\n\nfor x0 in x0_candidates:\n    x_final, f_final, success = newton_optimize(f_poly, f_poly_prime, f_poly_double_prime, x0)\n    grid_results.append((x0, x_final, f_final))\n    print(f\"{x0:5.2f}  →  {x_final:8.4f}  →  {f_final:8.4f}\")\n\nbest_grid = min(grid_results, key=lambda r: r[2])\nprint(f\"\\n★ 最良: x0={best_grid[0]:.2f} → x={best_grid[1]:.4f}, f(x)={best_grid[2]:.4f}\")\n\n\nグリッドサーチ：等間隔の15個の初期値\n=======================================================\n   x0  →      到着地点  →      f(x)\n-------------------------------------------------------\n-1.00  →   -0.0000  →    1.0000\n-0.43  →   -0.0000  →    1.0000\n 0.14  →   -0.0000  →    1.0000\n 0.71  →   -0.0000  →    1.0000\n 1.29  →    1.7344  →    2.7039\n 1.86  →    1.7344  →    2.7039\n 2.43  →    1.7344  →    2.7039\n 3.00  →    1.7344  →    2.7039\n 3.57  →    5.7656  →  -13.6726\n 4.14  →   -0.0000  →    1.0000\n 4.71  →    5.7656  →  -13.6726\n 5.29  →    5.7656  →  -13.6726\n 5.86  →    5.7656  →  -13.6726\n 6.43  →    5.7656  →  -13.6726\n 7.00  →    5.7656  →  -13.6726\n\n★ 最良: x0=3.57 → x=5.7656, f(x)=-13.6726\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nランダムリスタート vs グリッドサーチ\n\n\n\n\n\n\n\n\n\n\n\n方法\nやり方\n長所\n短所\n\n\n\n\nランダムリスタート\n初期値をランダムに選ぶ\n簡単、偏りが少ない\n運に左右される\n\n\nグリッドサーチ\n初期値を等間隔に並べる\n漏れが少ない\n数が多いと大変\n\n\n\nどちらも「何回も試して一番良いのを選ぶ」という同じ考え方です。"
  },
  {
    "objectID": "optimization-local-minima-appendix.html#実装テンプレート",
    "href": "optimization-local-minima-appendix.html#実装テンプレート",
    "title": "ローカルミニマムと初期値",
    "section": "実装テンプレート",
    "text": "実装テンプレート\n上の4ステップをまとめたコードです。コピーして使えます。\n\ndef robust_optimization(f, fp, fpp, x0_list, max_iter=100):\n    \"\"\"\n    複数の初期値を使ったロバストな最適化\n    \n    Parameters:\n    -----------\n    f, fp, fpp : functions\n        目的関数とその1次微分、2次微分\n    x0_list : list\n        初期値のリスト\n    max_iter : int\n        最大反復数\n    \n    Returns:\n    --------\n    best_x : float\n        見つかった最良解\n    all_results : list\n        比較用のすべての結果\n    \"\"\"\n    all_results = []\n    \n    for x0 in x0_list:\n        x = x0\n        \n        for i in range(max_iter):\n            grad = fp(x)\n            hess = fpp(x)\n            \n            if abs(grad) &lt; 1e-6:\n                break\n            if abs(hess) &lt; 1e-10:\n                break\n            \n            x = x - grad / hess\n        \n        all_results.append({\n            'x0': x0,\n            'x_final': x,\n            'f_final': f(x),\n            'is_minimum': fpp(x) &gt; 0\n        })\n    \n    # 谷（最小値）だけを残して、一番良いのを選ぶ\n    minima = [r for r in all_results if r['is_minimum']]\n    if minima:\n        best_result = min(minima, key=lambda r: r['f_final'])\n    else:\n        best_result = min(all_results, key=lambda r: r['f_final'])\n    \n    return best_result['x_final'], all_results\n\n# 使用例：5つの初期値から試す\ninitial_values = [-1, 0, 2, 4, 6]\nbest_x, all_results = robust_optimization(f_poly, f_poly_prime, f_poly_double_prime, \n                                          initial_values)\n\nprint(\"すべての結果:\")\nprint(f\"{'x0':&gt;4}  →  {'到着地点':&gt;8}  →  {'f(x)':&gt;8}  →  {'谷?':&gt;4}\")\nprint(\"-\" * 50)\nfor r in all_results:\n    valley = \"はい\" if r['is_minimum'] else \"いいえ\"\n    marker = \" ← 最良！\" if r['x_final'] == best_x else \"\"\n    print(f\"{r['x0']:4.1f}  →  {r['x_final']:8.4f}  →  {r['f_final']:8.4f}  →  {valley}{marker}\")\n\nprint(f\"\\n★ 最良解: x = {best_x:.4f}, f(x) = {f_poly(best_x):.4f}\")\n\nすべての結果:\n  x0  →      到着地点  →      f(x)  →    谷?\n--------------------------------------------------\n-1.0  →   -0.0000  →    1.0000  →  はい\n 0.0  →    0.0000  →    1.0000  →  はい\n 2.0  →    1.7344  →    2.7039  →  いいえ\n 4.0  →   -0.0000  →    1.0000  →  はい\n 6.0  →    5.7656  →  -13.6726  →  はい ← 最良！\n\n★ 最良解: x = 5.7656, f(x) = -13.6726"
  },
  {
    "objectID": "classification-2d-boundaries.html#第二回2次元での分類",
    "href": "classification-2d-boundaries.html#第二回2次元での分類",
    "title": "機械学習：分類問題",
    "section": "第二回：2次元での分類",
    "text": "第二回：2次元での分類\n目標\n\n2つの特徴量を使った分類\n手動で直線を引く → 決定木 → ロジスティック回帰の順で理解\n決定境界を可視化する\n\n\n\n\n\n\n\n\nここがポイント\n\n\n\n1次元（重さのみ）から2次元（重さ+輝度）への拡張\nまず目で見て、次にアルゴリズムで"
  },
  {
    "objectID": "classification-2d-boundaries.html#前回の復習",
    "href": "classification-2d-boundaries.html#前回の復習",
    "title": "機械学習：分類問題",
    "section": "前回の復習",
    "text": "前回の復習\n1次元での分類\n\n重さだけで Salmon vs Sea bass を分類\nしきい値（threshold）を手動で設定\nTP/FP/TN/FN、Precision/Recall を学習\n\n\n\n\n\n\n\nノート\n\n\n問題点: 最適なしきい値は、ケースによって異なる（コスト計算などで決定）"
  },
  {
    "objectID": "classification-2d-boundaries.html#今回のアプローチ",
    "href": "classification-2d-boundaries.html#今回のアプローチ",
    "title": "機械学習：分類問題",
    "section": "今回のアプローチ",
    "text": "今回のアプローチ\nもう1つ特徴量を追加する\n\nベルトコンベアに人感センサを搭載して、輝度（明るさ）も取れることがわかった\n\n重さや輝度のことを特徴量（feature）といいます\n\n2つの特徴量を使えば、もっと正確に分類できるはず？\n\n\n\n\n\n前回\n今回\n\n\n\n\n特徴量\n重さ（1次元）\n重さ + 輝度（2次元）\n\n\n境界\nしきい値\n？？"
  },
  {
    "objectID": "classification-2d-boundaries.html#データの準備",
    "href": "classification-2d-boundaries.html#データの準備",
    "title": "機械学習：分類問題",
    "section": "データの準備",
    "text": "データの準備\n新しく輝度のデータを追加しました（各40匹）。\n\n# Salmon（40匹）の重さ (g)\nsalmon_weights = [2149, 1959, 2194, 2457, 1930, 1930, 2474, 2230, 1859, 2163,\n                  1861, 1860, 2073, 1426, 1483, 1831, 1696, 2094, 1728, 1576,\n                  2028, 1903, 2057, 2229, 1884, 1884, 2240, 2081, 1838, 2037,\n                  1839, 1838, 1978, 1554, 1591, 1820, 1731, 1992, 1752, 1652]\n\n# Salmon（40匹）の輝度（暗め: 2.1-5.9）\nsalmon_brightness = [3.5, 5.8, 4.9, 4.4, 2.6, 2.6, 2.2, 5.5, 4.4, 4.8, 2.1, \n                     5.9, 5.3, 2.8, 2.7, 2.7, 3.2, 4.1, 3.7, 3.2, 4.4, 2.6, \n                     3.2, 3.5, 3.8, 5.1, 2.8, 4.1, 4.4, 2.2, 4.4, 2.7, 2.3, \n                     5.8, 5.9, 5.2, 3.2, 2.4, 4.7, 3.8]\n\n# Sea bass（40匹）の重さ (g)\nseabass_weights = [1677, 1327, 1486, 1440, 1857, 1476, 1749, 2203, 1979, 1468,\n                   1496, 1737, 1099, 1341, 1748, 1563, 2181, 1414, 1286, 1333,\n                   1787, 1445, 1505, 1204, 1381, 1513, 1259, 1567, 1370, 1432,\n                   1370, 1864, 1488, 1278, 1657, 1245, 1533, 1096, 1223, 1531]\n\n# Sea bass（40匹）の輝度（明るめ: 5.0-9.9）\nseabass_brightness = [5.6, 7.5, 5.2, 9.5, 6.3, 8.3, 6.6, 7.6, 7.7, 5.9, 9.8, \n                       8.9, 9.7, 9.5, 8.0, 9.6, 5.4, 6.0, 5.2, 6.6, 6.9, 6.4, \n                       9.1, 6.8, 6.4, 7.7, 5.7, 9.0, 5.4, 9.9, 8.9, 6.0, 5.0, \n                       9.1, 8.5, 8.6, 8.9, 5.4, 6.8, 5.6]"
  },
  {
    "objectID": "classification-2d-boundaries.html#準備",
    "href": "classification-2d-boundaries.html#準備",
    "title": "機械学習：分類問題",
    "section": "準備",
    "text": "準備\n必要なライブラリをインポートします。\n\n\n全体を表示\n# Setup\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n\n# Matplotlib settings\nplt.rcParams['font.size'] = 11\nplt.rcParams['figure.dpi'] = 100\n\n# データをnumpy配列に変換\nsalmon_weights = np.array(salmon_weights)\nsalmon_brightness = np.array(salmon_brightness)\nseabass_weights = np.array(seabass_weights)\nseabass_brightness = np.array(seabass_brightness)\n\n# 特徴量行列とラベルを作成\nX = np.vstack([\n    np.column_stack([salmon_brightness, salmon_weights]),\n    np.column_stack([seabass_brightness, seabass_weights])\n])\ny = np.array([1]*40 + [0]*40)  # 1=Salmon, 0=Sea bass"
  },
  {
    "objectID": "classification-2d-boundaries.html#目で見て境界線を想像垂線とする",
    "href": "classification-2d-boundaries.html#目で見て境界線を想像垂線とする",
    "title": "機械学習：分類問題",
    "section": "目で見て境界線を想像（垂線とする）",
    "text": "目で見て境界線を想像（垂線とする）\n\n\ncode\nplt.figure(figsize=(10, 7))\n\nplt.scatter(salmon_brightness, salmon_weights, \n           c='black', marker='o', s=100, alpha=0.7, label='Salmon', edgecolors='black')\nplt.scatter(seabass_brightness, seabass_weights, \n           c='red', marker='s', s=100, alpha=0.7, label='Sea bass', edgecolors='darkred')\n\nplt.xlabel('Lightness', fontsize=14, fontweight='bold')\nplt.ylabel('Weight (g)', fontsize=14, fontweight='bold')\nplt.title('2D Feature Space: Lightness vs Weight', fontsize=16, fontweight='bold')\nplt.legend(fontsize=13, loc='upper left')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n観察\n\n\n\nSalmon（黒丸）は左側（暗い）に集中\nSea bass（赤四角）は右側（明るい）に集中\nでも5〜6あたりで少し重なっている\n斜めの直線で分けられそう！"
  },
  {
    "objectID": "classification-2d-boundaries.html#試行1-目で見て直線を引いてみる",
    "href": "classification-2d-boundaries.html#試行1-目で見て直線を引いてみる",
    "title": "機械学習：分類問題",
    "section": "試行1: 目で見て直線を引いてみる",
    "text": "試行1: 目で見て直線を引いてみる\n考え方\nグラフを見て、手動で直線を引いてみましょう。\n\nどこに引けば一番うまく分けられそうか？\n完璧には分けられない → どのくらい誤分類が出るか？"
  },
  {
    "objectID": "classification-2d-boundaries.html#この辺だとどうか",
    "href": "classification-2d-boundaries.html#この辺だとどうか",
    "title": "機械学習：分類問題",
    "section": "この辺だとどうか",
    "text": "この辺だとどうか\n\n\ncode\nplt.figure(figsize=(10, 7))\n\nplt.scatter(salmon_brightness, salmon_weights, \n           c='black', marker='o', s=100, alpha=0.7, label='Salmon', edgecolors='black')\nplt.scatter(seabass_brightness, seabass_weights, \n           c='red', marker='s', s=100, alpha=0.7, label='Sea bass', edgecolors='darkred')\n\n# 手動で引いた直線の例（適当に設定）\n# 例: 輝度=5.5あたりで垂直に分ける\nlightness_threshold = 5.5\nplt.axvline(x=lightness_threshold, color='blue', linestyle='--', linewidth=2.5, \n           label=f'decision boundary (Lightness = {lightness_threshold})')\n\nplt.xlabel('Lightness', fontsize=14, fontweight='bold')\nplt.ylabel('Weight (g)', fontsize=14, fontweight='bold')\nplt.title('Manual Decision Boundary', fontsize=16, fontweight='bold')\nplt.legend(fontsize=13, loc='upper left')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nこの直線の性能は？\n\n\n輝度5.5を境界にすると、どのくらいの精度になるでしょうか？ 自分で計算してみましょう。\nSalmon を陽性 (1) とし、輝度 &lt; 5.5 → Salmon、&gt;= 5.5 → Sea bass で判定すると以下の通り。\n\n\n\n\n予測Salmon\n予測Sea bass\n\n\n\n\n実際Salmon\nTP = 35\nFN = 5\n\n\n実際Sea bass\nFP = 6\nTN = 34\n\n\n\n\nAccuracy = (35 + 34) / 80 = 0.86\nPrecision (Salmon) = 35 / (35 + 6) = 0.85\nRecall (Salmon) = 35 / (35 + 5) = 0.88\nF1 = 2TP / (2TP + FP + FN) = 70 / 81 ≈ 0.86\n\nSea bass を陽性にする等、別の定義を使う場合は同じ手順で数え直してみましょう。"
  },
  {
    "objectID": "classification-2d-boundaries.html#試行2-決定木decision-tree",
    "href": "classification-2d-boundaries.html#試行2-決定木decision-tree",
    "title": "機械学習：分類問題",
    "section": "試行2: 決定木（Decision Tree）",
    "text": "試行2: 決定木（Decision Tree）\nまず：ジニ不純度とは？\n決定木は「どこで分割すれば一番きれいに分かれるか」を自動で探します。\nその基準がジニ不純度（Gini Impurity）です。\n\\[\\text{Gini} = 1 - (p_{\\text{Salmon}}^2 + p_{\\text{Sea bass}}^2)\\]\n\nGini = 0: 完全に1クラスだけ（純粋）\nGini = 0.5: 50:50で混ざってる（不純）\n\n\n\n\n\n\n\n例\n\n\n\nデータ10個：Salmon 10個、Sea bass 0個 → Gini = 1 - (1² + 0²) = 0\nデータ10個：Salmon 5個、Sea bass 5個 → Gini = 1 - (0.5² + 0.5²) = 0.5\nデータ10個：Salmon 8個、Sea bass 2個 → Gini = 1 - (0.8² + 0.2²) = 0.32"
  },
  {
    "objectID": "classification-2d-boundaries.html#ジニ不純度を計算してみる",
    "href": "classification-2d-boundaries.html#ジニ不純度を計算してみる",
    "title": "機械学習：分類問題",
    "section": "ジニ不純度を計算してみる",
    "text": "ジニ不純度を計算してみる\n\n\ncode\ndef gini_impurity(labels):\n    \"\"\"ジニ不純度を計算\"\"\"\n    if len(labels) == 0:\n        return 0\n    p_salmon = sum(labels) / len(labels)  # 1=Salmon, 0=Sea bass\n    p_seabass = 1 - p_salmon\n    return 1 - (p_salmon**2 + p_seabass**2)\n\n# 全データのジニ不純度\ninitial_gini = gini_impurity(y)\nprint(f\"初期状態（分割前）のジニ不純度: {initial_gini:.3f}\")\nprint(f\"  Salmon: {sum(y)}個, Sea bass: {len(y)-sum(y)}個\")\n\n\n初期状態（分割前）のジニ不純度: 0.500\n  Salmon: 40個, Sea bass: 40個"
  },
  {
    "objectID": "classification-2d-boundaries.html#最良の閾値をどう探す",
    "href": "classification-2d-boundaries.html#最良の閾値をどう探す",
    "title": "機械学習：分類問題",
    "section": "最良の閾値をどう探す？",
    "text": "最良の閾値をどう探す？\n方法：総当たり\nデータの値すべてを候補として試します。\n\n\n\n\n\n\n\nなぜデータの値？\n\n\nデータとデータの間で分けるので、データの値を試せば十分。\n例：データが [2.1, 3.5, 5.8] なら - 2.1と3.5の間で分ける = 2.1で分割と同じ結果 - 3.5と5.8の間で分ける = 3.5で分割と同じ結果\n→ データの値だけ試せばOK"
  },
  {
    "objectID": "classification-2d-boundaries.html#いくつかの閾値を試してみる",
    "href": "classification-2d-boundaries.html#いくつかの閾値を試してみる",
    "title": "機械学習：分類問題",
    "section": "いくつかの閾値を試してみる",
    "text": "いくつかの閾値を試してみる\n\n\ncode\n# 輝度でいくつかの閾値を試す\ntest_thresholds = [3.0, 4.0, 5.0, 6.0, 7.0]\nresults = []\n\nfor threshold in test_thresholds:\n    # 分割\n    left_mask = X[:, 0] &lt; threshold\n    right_mask = ~left_mask\n    \n    if sum(left_mask) == 0 or sum(right_mask) == 0:\n        continue\n    \n    # 左右のジニ不純度\n    gini_left = gini_impurity(y[left_mask])\n    gini_right = gini_impurity(y[right_mask])\n    \n    # 加重平均\n    n_left = sum(left_mask)\n    n_right = sum(right_mask)\n    weighted_gini = (n_left * gini_left + n_right * gini_right) / len(y)\n    \n    results.append({\n        '閾値': f'{threshold:.1f}',\n        '左側': f'{sum(y[left_mask])}/{n_left}',\n        '右側': f'{sum(y[right_mask])}/{n_right}',\n        'Gini_left': f'{gini_left:.3f}',\n        'Gini_right': f'{gini_right:.3f}',\n        '加重平均': f'{weighted_gini:.3f}'\n    })\n\nresults_df = pd.DataFrame(results)\nprint(\"いくつかの閾値でのジニ不純度:\")\ndisplay(results_df)\nprint(\"\\n→ 加重平均が最小の閾値を選ぶ\")\n\n\nいくつかの閾値でのジニ不純度:\n\n\n\n\n\n\n\n\n\n閾値\n左側\n右側\nGini_left\nGini_right\n加重平均\n\n\n\n\n0\n3.0\n13/13\n27/67\n0.000\n0.481\n0.403\n\n\n1\n4.0\n22/22\n18/58\n0.000\n0.428\n0.310\n\n\n2\n5.0\n32/32\n8/48\n0.000\n0.278\n0.167\n\n\n3\n6.0\n40/50\n0/30\n0.320\n0.000\n0.200\n\n\n4\n7.0\n40/60\n0/20\n0.444\n0.000\n0.333\n\n\n\n\n\n\n\n\n→ 加重平均が最小の閾値を選ぶ\n\n\n\n\n\n\n\n\n重要\n\n\n実際は全てのユニーク値（80個以上）を試して、最小を見つけます。"
  },
  {
    "objectID": "classification-2d-boundaries.html#決定木のアルゴリズム交互版",
    "href": "classification-2d-boundaries.html#決定木のアルゴリズム交互版",
    "title": "機械学習：分類問題",
    "section": "決定木のアルゴリズム（交互版）",
    "text": "決定木のアルゴリズム（交互版）\nルール：特徴量を交互に使う\n1回目は輝度、2回目は重さ、3回目は輝度…と交互に使うことで、階段状の境界を作ります。\n\n\ncode\ndef find_best_split_for_feature(X, y, feature_idx):\n    \"\"\"指定した特徴量で最良の分割点を見つける\"\"\"\n    best_gini = float('inf')\n    best_threshold = None\n    best_left_mask = None\n    \n    # この特徴量のユニークな値を試す\n    values = np.unique(X[:, feature_idx])\n    \n    for threshold in values:\n        # 分割\n        left_mask = X[:, feature_idx] &lt; threshold\n        right_mask = ~left_mask\n        \n        # 左右が空でないことを確認\n        if sum(left_mask) == 0 or sum(right_mask) == 0:\n            continue\n        \n        # 左右のジニ不純度\n        gini_left = gini_impurity(y[left_mask])\n        gini_right = gini_impurity(y[right_mask])\n        \n        # 加重平均\n        n_left = sum(left_mask)\n        n_right = sum(right_mask)\n        weighted_gini = (n_left * gini_left + n_right * gini_right) / len(y)\n        \n        if weighted_gini &lt; best_gini:\n            best_gini = weighted_gini\n            best_threshold = threshold\n            best_left_mask = left_mask\n    \n    return best_threshold, best_gini, best_left_mask\n\n# 1回目の分割: 輝度（feature 0）\nfeature_names = ['Lightness', 'Weight']\nsplit1_feature = 0  # Lightness\nsplit1_threshold, split1_gini, split1_left_mask = find_best_split_for_feature(X, y, split1_feature)\n\nprint(f\"1回目の分割:\")\nprint(f\"  特徴量: {feature_names[split1_feature]}\")\nprint(f\"  閾値: {split1_threshold:.2f}\")\nprint(f\"  ジニ不純度: {split1_gini:.3f}\")\nprint(f\"  左側: {sum(split1_left_mask)}個, 右側: {sum(~split1_left_mask)}個\")\n\n\n1回目の分割:\n  特徴量: Lightness\n  閾値: 5.00\n  ジニ不純度: 0.167\n  左側: 32個, 右側: 48個"
  },
  {
    "objectID": "classification-2d-boundaries.html#可視化1回目の分割",
    "href": "classification-2d-boundaries.html#可視化1回目の分割",
    "title": "機械学習：分類問題",
    "section": "可視化：1回目の分割",
    "text": "可視化：1回目の分割"
  },
  {
    "objectID": "classification-2d-boundaries.html#ステップ2-2回目の分割重さで分ける",
    "href": "classification-2d-boundaries.html#ステップ2-2回目の分割重さで分ける",
    "title": "機械学習：分類問題",
    "section": "ステップ2: 2回目の分割（重さで分ける）",
    "text": "ステップ2: 2回目の分割（重さで分ける）\n左側と右側をそれぞれ重さで分割します。\n\n\ncode\nsplit2_feature = 1  # Weight（交互なので重さ）\n\n# 左側を重さで分割\nX_left = X[split1_left_mask]\ny_left = y[split1_left_mask]\n\nsplit2_left_threshold = None\nif len(X_left) &gt; 1:\n    split2_left_threshold, split2_left_gini, split2_left_left_mask = find_best_split_for_feature(X_left, y_left, split2_feature)\n    print(f\"\\n2回目の分割（左側）:\")\n    print(f\"  特徴量: {feature_names[split2_feature]}\")\n    print(f\"  閾値: {split2_left_threshold:.0f}\")\n    print(f\"  ジニ不純度: {split2_left_gini:.3f}\")\n\n# 右側を重さで分割\nX_right = X[~split1_left_mask]\ny_right = y[~split1_left_mask]\n\nsplit2_right_threshold = None\nif len(X_right) &gt; 1:\n    split2_right_threshold, split2_right_gini, split2_right_left_mask = find_best_split_for_feature(X_right, y_right, split2_feature)\n    print(f\"\\n2回目の分割（右側）:\")\n    print(f\"  特徴量: {feature_names[split2_feature]}\")\n    print(f\"  閾値: {split2_right_threshold:.0f}\")\n    print(f\"  ジニ不純度: {split2_right_gini:.3f}\")\n\n\n\n2回目の分割（左側）:\n  特徴量: Weight\n  閾値: 1483\n  ジニ不純度: 0.000\n\n2回目の分割（右側）:\n  特徴量: Weight\n  閾値: 1820\n  ジニ不純度: 0.192"
  },
  {
    "objectID": "classification-2d-boundaries.html#可視化2回目の分割",
    "href": "classification-2d-boundaries.html#可視化2回目の分割",
    "title": "機械学習：分類問題",
    "section": "可視化：2回目の分割",
    "text": "可視化：2回目の分割"
  },
  {
    "objectID": "classification-2d-boundaries.html#この境界線の性能は",
    "href": "classification-2d-boundaries.html#この境界線の性能は",
    "title": "機械学習：分類問題",
    "section": "この境界線の性能は？",
    "text": "この境界線の性能は？\n階段状の境界線で分類した結果を評価します。\n\n\ncode\n# 簡易的な予測関数\ndef predict_alternating(brightness, weight):\n    \"\"\"交互分割の決定木で予測\"\"\"\n    if brightness &lt; split1_threshold:\n        # 左側（暗い方＝Salmon寄り）\n        if split2_left_threshold is not None:\n            if weight &gt;= split2_left_threshold:\n                return 1  # Salmon（重い）\n            else:\n                return 0  # Sea bass（軽い）\n        else:\n            return 1  # Salmon (分割なし)\n    else:\n        # 右側（明るい方＝Sea bass寄り）\n        if split2_right_threshold is not None:\n            if weight &gt;= split2_right_threshold:\n                return 1  # Salmon（重い）\n            else:\n                return 0  # Sea bass（軽い）\n        else:\n            return 0  # Sea bass (分割なし)\n\n# 全データで予測\ny_pred_alternating = np.array([predict_alternating(X[i, 0], X[i, 1]) for i in range(len(X))])\n\n# 混同行列\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y, y_pred_alternating)\n\nTP = cm[1, 1]\nTN = cm[0, 0]\nFP = cm[0, 1]\nFN = cm[1, 0]\n\n# 評価指標\naccuracy_alt = (TP + TN) / (TP + TN + FP + FN)\nprecision_alt = TP / (TP + FP) if (TP + FP) &gt; 0 else 0\nrecall_alt = TP / (TP + FN) if (TP + FN) &gt; 0 else 0\n\nprint(f\"交互分割決定木の性能 (深さ2):\")\nprint(f\"  TP={TP}, FP={FP}, FN={FN}, TN={TN}\")\nprint(f\"  Accuracy:  {accuracy_alt:.3f}\")\nprint(f\"  Precision: {precision_alt:.3f}\")\nprint(f\"  Recall:    {recall_alt:.3f}\")\n\n\n交互分割決定木の性能 (深さ2):\n  TP=37, FP=5, FN=3, TN=35\n  Accuracy:  0.900\n  Precision: 0.881\n  Recall:    0.925"
  },
  {
    "objectID": "classification-2d-boundaries.html#scikit-learnの決定木と比較",
    "href": "classification-2d-boundaries.html#scikit-learnの決定木と比較",
    "title": "機械学習：分類問題",
    "section": "scikit-learnの決定木と比較",
    "text": "scikit-learnの決定木と比較\n自作の決定木（深さ2、交互分割）とライブラリの実装（深さ3、最良特徴量を選択）を比較してみましょう。\n\n\n\n\n\n\n違い\n\n\n\n自作版: 深さ2まで、輝度→重さと交互に使う\nscikit-learn: 深さ3まで、毎回最良の特徴量を自動選択\n→ scikit-learnの方がより細かい階段になる\n\n\n\n\n\n\ncode\n# 決定木モデル（深さ3まで）\ntree_model = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_model.fit(X, y)\n\n# 予測\ny_pred_tree = tree_model.predict(X)\n\n# 評価\naccuracy_tree = accuracy_score(y, y_pred_tree)\nprecision_tree = precision_score(y, y_pred_tree)\nrecall_tree = recall_score(y, y_pred_tree)\n\nprint(f\"決定木の性能:\")\nprint(f\"  Accuracy:  {accuracy_tree:.3f}\")\nprint(f\"  Precision: {precision_tree:.3f}\")\nprint(f\"  Recall:    {recall_tree:.3f}\")\n\n\n決定木の性能:\n  Accuracy:  0.975\n  Precision: 0.952\n  Recall:    1.000"
  },
  {
    "objectID": "classification-2d-boundaries.html#決定境界の可視化決定木",
    "href": "classification-2d-boundaries.html#決定境界の可視化決定木",
    "title": "機械学習：分類問題",
    "section": "決定境界の可視化（決定木）",
    "text": "決定境界の可視化（決定木）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nノート\n\n\n\n決定境界が階段状（水平・垂直の線の組み合わせ）\nif-thenルールの結果がこの形になる"
  },
  {
    "objectID": "classification-2d-boundaries.html#試行3-ロジスティック回帰線形分類器",
    "href": "classification-2d-boundaries.html#試行3-ロジスティック回帰線形分類器",
    "title": "機械学習：分類問題",
    "section": "試行3: ロジスティック回帰（線形分類器）",
    "text": "試行3: ロジスティック回帰（線形分類器）\n考え方\n数学的に最適な直線を見つける\n\n数式: \\(w_1 \\times \\text{輝度} + w_2 \\times \\text{重さ} + b = 0\\)\nこの直線で2クラスを最もうまく分ける\n斜めの直線が引ける\n\n\n\n\n\n\n\n\nロジスティック回帰\n\n\n線形分類器の代表的な手法。確率も計算できる。"
  },
  {
    "objectID": "classification-2d-boundaries.html#ロジスティック回帰で学習",
    "href": "classification-2d-boundaries.html#ロジスティック回帰で学習",
    "title": "機械学習：分類問題",
    "section": "ロジスティック回帰で学習",
    "text": "ロジスティック回帰で学習\n\n\ncode\n# ロジスティック回帰モデル\nlr_model = LogisticRegression()\nlr_model.fit(X, y)\n\n# 予測\ny_pred_lr = lr_model.predict(X)\n\n# 評価\naccuracy_lr = accuracy_score(y, y_pred_lr)\nprecision_lr = precision_score(y, y_pred_lr)\nrecall_lr = recall_score(y, y_pred_lr)\n\nprint(f\"ロジスティック回帰の性能:\")\nprint(f\"  Accuracy:  {accuracy_lr:.3f}\")\nprint(f\"  Precision: {precision_lr:.3f}\")\nprint(f\"  Recall:    {recall_lr:.3f}\")\n\n\nロジスティック回帰の性能:\n  Accuracy:  0.950\n  Precision: 0.974\n  Recall:    0.925"
  },
  {
    "objectID": "classification-2d-boundaries.html#決定境界の可視化ロジスティック回帰",
    "href": "classification-2d-boundaries.html#決定境界の可視化ロジスティック回帰",
    "title": "機械学習：分類問題",
    "section": "決定境界の可視化（ロジスティック回帰）",
    "text": "決定境界の可視化（ロジスティック回帰）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nノート\n\n\n\n青い直線が決定境界\n背景色: 赤＝Salmon領域、青＝Sea bass領域\n斜めの直線1本でシンプルに分離"
  },
  {
    "objectID": "classification-2d-boundaries.html#つのアルゴリズムの比較",
    "href": "classification-2d-boundaries.html#つのアルゴリズムの比較",
    "title": "機械学習：分類問題",
    "section": "2つのアルゴリズムの比較",
    "text": "2つのアルゴリズムの比較\n\n\n決定木\nメリット: - 解釈しやすい（if-thenルール） - 自動で最適化（ジニ不純度） - 軸に平行な境界\nデメリット: - 過学習しやすい - 階段状の境界のみ\n\nロジスティック回帰\nメリット: - 数学的に最適 - シンプル（斜めの直線） - 過学習しにくい\nデメリット: - 線形のみ - 複雑なパターンに弱い"
  },
  {
    "objectID": "classification-2d-boundaries.html#性能比較表",
    "href": "classification-2d-boundaries.html#性能比較表",
    "title": "機械学習：分類問題",
    "section": "性能比較表",
    "text": "性能比較表\n\n\n\n\n\n\nモデル\nAccuracy\nPrecision\nRecall\n\n\n\n\n決定木\n0.975\n0.952\n1.000\n\n\nロジスティック回帰\n0.950\n0.974\n0.925\n\n\n\n\n\n\n\n\n\n\n\n\n重要\n\n\nどちらが「正解」ではなく、データと目的に応じて選ぶ"
  },
  {
    "objectID": "classification-2d-boundaries.html#決定木の深さを変えると",
    "href": "classification-2d-boundaries.html#決定木の深さを変えると",
    "title": "機械学習：分類問題",
    "section": "決定木の深さを変えると？",
    "text": "決定木の深さを変えると？\n深さを変えることで、複雑さをコントロールできます。\n\n\ncode\ndepths = [1, 3, 5]\nresults = []\n\nfor depth in depths:\n    model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    model.fit(X, y)\n    y_pred = model.predict(X)\n    \n    acc = accuracy_score(y, y_pred)\n    prec = precision_score(y, y_pred)\n    rec = recall_score(y, y_pred)\n    \n    results.append({\n        '深さ': depth,\n        'Accuracy': f'{acc:.3f}',\n        'Precision': f'{prec:.3f}',\n        'Recall': f'{rec:.3f}'\n    })\n\ndepth_df = pd.DataFrame(results)\ndepth_df\n\n\n\n\n\n\n\n\n\n深さ\nAccuracy\nPrecision\nRecall\n\n\n\n\n0\n1\n0.900\n1.000\n0.800\n\n\n1\n3\n0.975\n0.952\n1.000\n\n\n2\n5\n0.988\n1.000\n0.975\n\n\n\n\n\n\n\n\n\n\n\n\n\n警告\n\n\n\n深さ1: シンプルすぎる（underfitting）\n深さ3: バランスが良い\n深さ5: 複雑すぎる可能性（overfitting のリスク）"
  },
  {
    "objectID": "classification-2d-boundaries.html#過学習overfittingとは",
    "href": "classification-2d-boundaries.html#過学習overfittingとは",
    "title": "機械学習：分類問題",
    "section": "過学習（Overfitting）とは？",
    "text": "過学習（Overfitting）とは？\n\n\n適切な複雑さ - 訓練データによく適合 - 新しいデータにも対応\n\n過学習 - 訓練データに過剰適合 - 新しいデータでは性能低下\n\n\n\n\n\n\n\n\n対策\n\n\n\n決定木の深さを制限\nデータを増やす\n交差検証で評価"
  },
  {
    "objectID": "classification-2d-boundaries.html#混同行列で詳しく見る決定木",
    "href": "classification-2d-boundaries.html#混同行列で詳しく見る決定木",
    "title": "機械学習：分類問題",
    "section": "混同行列で詳しく見る（決定木）",
    "text": "混同行列で詳しく見る（決定木）\n\n\ncode\n# 決定木（深さ3）の混同行列\ncm = confusion_matrix(y, y_pred_tree)\n\nTP = cm[1, 1]  # Salmon correctly classified as Salmon\nTN = cm[0, 0]  # Sea bass correctly classified as Sea bass\nFP = cm[0, 1]  # Sea bass incorrectly classified as Salmon\nFN = cm[1, 0]  # Salmon incorrectly classified as Sea bass\n\nimport matplotlib.patches as mpatches\n\nfig, ax = plt.subplots(figsize=(9, 7))\n\n# Confusion matrix data\ncm_data = np.array([[TP, FP], [FN, TN]])\n\n# Color map: 緑=正解、ピンク=誤り\ncolors = np.array([['#90EE90', '#FFB6C6'], ['#FFB6C6', '#90EE90']])\n\n# Draw cells\nfor i in range(2):\n    for j in range(2):\n        ax.add_patch(mpatches.Rectangle((j, 1-i), 1, 1, \n                                        facecolor=colors[i, j], \n                                        edgecolor='black', linewidth=2.5))\n        ax.text(j + 0.5, 1.5 - i, str(cm_data[i, j]), \n               ha='center', va='center', fontsize=48, fontweight='bold')\n        \n        # Add label\n        if i == 0 and j == 0:\n            label = 'TP'\n        elif i == 0 and j == 1:\n            label = 'FP'\n        elif i == 1 and j == 0:\n            label = 'FN'\n        else:\n            label = 'TN'\n        ax.text(j + 0.5, 1.15 - i, label, \n               ha='center', va='center', fontsize=16, style='italic', color='gray')\n\n# Set labels\nax.set_xlim(0, 2)\nax.set_ylim(0, 2)\nax.set_xticks([0.5, 1.5])\nax.set_yticks([0.5, 1.5])\nax.set_xticklabels(['Actual: Salmon', 'Actual: Sea bass'], fontsize=13, fontweight='bold')\nax.set_yticklabels(['Predict: Sea bass', 'Predict: Salmon'], fontsize=13, fontweight='bold')\nax.set_aspect('equal')\nax.tick_params(length=0)\nax.set_title('Confusion Matrix (Decision Tree)', \n            fontsize=16, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTP={TP}, FP={FP}, FN={FN}, TN={TN}\")\n\n\n\n\n\n\n\n\n\n\nTP=40, FP=2, FN=0, TN=38"
  },
  {
    "objectID": "classification-2d-boundaries.html#第二回の結論",
    "href": "classification-2d-boundaries.html#第二回の結論",
    "title": "機械学習：分類問題",
    "section": "第二回の結論",
    "text": "第二回の結論\n\n\n\n\n\n\n重要なポイント\n\n\n\n2次元にすることで分類精度が向上\n\n重さ + 輝度の組み合わせで、より正確に分類\n\nまず目で見て、次にアルゴリズム\n\n手動の直線 → 決定木 → ロジスティック回帰\n段階的に理解を深める\n\nアルゴリズムによって決定境界が異なる\n\n決定木: 階段状（軸に平行）\nロジスティック回帰: 斜めの直線\n\nトレードオフを理解する\n\nシンプル vs 複雑\n解釈しやすさ vs 性能\n過学習のリスク"
  },
  {
    "objectID": "classification-2d-boundaries.html#練習問題",
    "href": "classification-2d-boundaries.html#練習問題",
    "title": "機械学習：分類問題",
    "section": "練習問題",
    "text": "練習問題\n以下を実装して、レポートで提出せよ。\n\n手動で境界線を決める\n\nグラフを見て、自分で境界線を決める\nその境界線でのTP, FP, FN, TNを計算\n\n決定木と ロジスティック回帰を両方実装\n\n提供されたデータで学習\n\n決定木の深さを1, 2, 3, 4, 5で試す\n\nそれぞれのAccuracy, Precision, Recallを比較\nどの深さが最適か考察\n\n考察\n\n手動の境界線 vs 決定木 vs ロジスティック回帰\nどれが最も良かったか？その理由は？\n2次元にすることで、1次元と比べてどう改善したか？"
  },
  {
    "objectID": "classification-2d-boundaries.html#次回予告",
    "href": "classification-2d-boundaries.html#次回予告",
    "title": "機械学習：分類問題",
    "section": "次回予告",
    "text": "次回予告\n第三回では…\n\n3種類の魚を扱う（多クラス分類）\nまたは教師なし学習（クラスタリング）\nより実践的な機械学習の世界へ！"
  },
  {
    "objectID": "classification-basics-1d.html#第一回分類問題",
    "href": "classification-basics-1d.html#第一回分類問題",
    "title": "機械学習：分類問題",
    "section": "第一回：分類問題",
    "text": "第一回：分類問題\n目標\n\n分類問題(Data Classification)：データを分類するタスクを理解する\n\n2クラス分類\n\nデータ可視化\n決定境界（decision boundary） の概念を学ぶ\n\n\n\n\n\n\n\n\nここがポイント\n\n\n\n完全な分類は困難であることを知る\n境界決定のトレードオフとコスト"
  },
  {
    "objectID": "classification-basics-1d.html#今回の問題",
    "href": "classification-basics-1d.html#今回の問題",
    "title": "機械学習：分類問題",
    "section": "今回の問題",
    "text": "今回の問題\nサケだけを選別せよ\n\n\n\n魚はベルトコンベアで大量に流れてくる\n\n時間をかけず選別したい\n\nサケ以外の魚も混じっている\n\n一旦、サケ（Salmon）かスズキ（Sea bass）か\n分類項目をクラス（class）という\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nここがポイント\n\n\nSalmon と Sea bass の2クラス識別(2-class classification)問題"
  },
  {
    "objectID": "classification-basics-1d.html#選別システム",
    "href": "classification-basics-1d.html#選別システム",
    "title": "機械学習：分類問題",
    "section": "選別システム",
    "text": "選別システム\n\n\n\n\n\n\n\nflowchart TB\n    A[ベルトコンベアに魚を流す]\n    B[途中で重さを測定]\n    D{何かしらの判定器}\n    E[鮭として加工]\n    F[スズキとして加工]\n\n    A --&gt; B --&gt; D\n    D --&gt;|Salmon| E\n    D --&gt;|Sea bass| F\n\n    classDef bigText font-size:16px,font-weight:400;\n    class A,B,D,E,F bigText;\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style D fill:#ff4d4f,color:#ffffff,stroke:#b22222,stroke-width:3px,font-weight:700\n    style E fill:#ffcccb\n    style F fill:#add8e6"
  },
  {
    "objectID": "classification-basics-1d.html#試行1-重さだけで分類してみる",
    "href": "classification-basics-1d.html#試行1-重さだけで分類してみる",
    "title": "機械学習：分類問題",
    "section": "試行1: 重さだけで分類してみる",
    "text": "試行1: 重さだけで分類してみる\nアプローチ\n\n重量計をつけられそうなので、まず重さで識別できないかを考えた\nまずサンプルとして20匹ずつの重さを測定した\n\n\n# Salmon（20匹）の重さ (g)\nsalmon_weights = [2149, 1959, 2194, 2457, 1930, 1930, 2474, 2230, 1859, 2163,\n                  1861, 1860, 2073, 1426, 1483, 1831, 1696, 2094, 1728, 1576]\n\n\n# Sea bass（20匹）の重さ (g)\nseabass_weights = [1677, 1327, 1486, 1440, 1857, 1476, 1749, 2203, 1979, 1468,\n                   1496, 1737, 1099, 1341, 1748, 1563, 2181, 1414, 1286, 1333]"
  },
  {
    "objectID": "classification-basics-1d.html#準備",
    "href": "classification-basics-1d.html#準備",
    "title": "機械学習：分類問題",
    "section": "準備",
    "text": "準備\npandas, numpy, matplotlib, scikit-learnをインストールします。\npip install pandas numpy matplotlib scikit-learn altair[all]\nGoogle Colabの場合は、altair[all]だけインストールすればOKです\n!pip install altair[all]\n以下を実行\n\n\n全体を表示\n# Setup\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\n# Matplotlib settings\nplt.rcParams['font.size'] = 11\nplt.rcParams['figure.dpi'] = 100\n\nsalmon_weights = np.array(salmon_weights)\nseabass_weights = np.array(seabass_weights)"
  },
  {
    "objectID": "classification-basics-1d.html#まずはヒストグラム",
    "href": "classification-basics-1d.html#まずはヒストグラム",
    "title": "機械学習：分類問題",
    "section": "まずはヒストグラム",
    "text": "まずはヒストグラム\n\n\ncode\nplt.figure(figsize=(10, 6))\nbins = range(1000, 2600, 100)\n\nplt.hist(salmon_weights, bins=bins, histtype='step', \n         edgecolor='black', linewidth=2, label=\"Salmon\")\nplt.hist(seabass_weights, bins=bins, histtype='step', \n         edgecolor='red', linewidth=2, label=\"Sea bass\")\n\nplt.xlabel(\"Weight (g)\", fontsize=14)\nplt.ylabel(\"Count\", fontsize=14)\nplt.title(\"Distribution of Fish Weight\", fontsize=16, fontweight='bold')\nplt.legend(fontsize=13)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "classification-basics-1d.html#統計値を見る",
    "href": "classification-basics-1d.html#統計値を見る",
    "title": "機械学習：分類問題",
    "section": "統計値を見る",
    "text": "統計値を見る\n\n\ncode\nprint(f\"Salmon   - 最小: {min(salmon_weights)}g, 最大: {max(salmon_weights)}g, 平均: {np.mean(salmon_weights):.0f}g\")\nprint(f\"Sea bass - 最小: {min(seabass_weights)}g, 最大: {max(seabass_weights)}g, 平均: {np.mean(seabass_weights):.0f}g\")\noverlap_min = max(min(salmon_weights), min(seabass_weights))\noverlap_max = min(max(salmon_weights), max(seabass_weights))\nif overlap_max &gt;= overlap_min:\n    print(f\"\\n重なり範囲: {overlap_min}g - {overlap_max}g\")\n\n\nSalmon   - 最小: 1426g, 最大: 2474g, 平均: 1949g\nSea bass - 最小: 1099g, 最大: 2203g, 平均: 1593g\n\n重なり範囲: 1426g - 2203g\n\n\n\n\n\n\n\n\n発見\n\n\n\n双峰型の分布：2つの山が見える\n\n重なっている がまあ分けれそう？"
  },
  {
    "objectID": "classification-basics-1d.html#境界線閾値を考える",
    "href": "classification-basics-1d.html#境界線閾値を考える",
    "title": "機械学習：分類問題",
    "section": "境界線（閾値）を考える",
    "text": "境界線（閾値）を考える\n大体でいいので 境界線（threshold） を設定します。\n方針\n1800g以上 → Salmon\n1800g未満 → Sea bass\nこの判定ルールで分類してみましょう。"
  },
  {
    "objectID": "classification-basics-1d.html#青破線で切ったとしたら",
    "href": "classification-basics-1d.html#青破線で切ったとしたら",
    "title": "機械学習：分類問題",
    "section": "青破線で切ったとしたら",
    "text": "青破線で切ったとしたら"
  },
  {
    "objectID": "classification-basics-1d.html#分類結果4つのパターン",
    "href": "classification-basics-1d.html#分類結果4つのパターン",
    "title": "機械学習：分類問題",
    "section": "分類結果：4つのパターン",
    "text": "分類結果：4つのパターン\n境界線1800gで分類すると、4つのパターンが生まれます：\n\n\n\n\n\n\n\n\n\n実際の魚\n判定結果\n略称\n説明\n\n\n\n\nSalmon\nSalmon\nTP (True Positive)\n正しくSalmonと判定\n\n\nSea bass\nSea bass\nTN (True Negative)\n正しくSalmonではないと判定\n\n\nSea bass\nSalmon\nFP (False Positive)\n誤ってSalmonと判定\n\n\nSalmon\nSea bass\nFN (False Negative)\nSalmonを見逃し"
  },
  {
    "objectID": "classification-basics-1d.html#tpfptnfnの覚え方",
    "href": "classification-basics-1d.html#tpfptnfnの覚え方",
    "title": "機械学習：分類問題",
    "section": "TP/FP/TN/FNの覚え方",
    "text": "TP/FP/TN/FNの覚え方\n右から左に読む\n\n右（P/N） 判定結果 → Positive (シャケ) or Negative (それ以外)\n左（T/F） その答え合わせ** → True (正解) or False (不正解)\n\nつまり、ある魚をみて識別した結果、\n\nTP: Positiveと判定 → True (正しかった！)\nFP: Positiveと判定 → False (間違ってた…)\nTN: Negatigeと判定 → True (正しかった！)\nFN: Negativeと判定 → False (間違ってた…)"
  },
  {
    "objectID": "classification-basics-1d.html#混同行列confusion-matrix",
    "href": "classification-basics-1d.html#混同行列confusion-matrix",
    "title": "機械学習：分類問題",
    "section": "混同行列（Confusion Matrix）",
    "text": "混同行列（Confusion Matrix）\n\n\ncode\nthreshold = 1800\n\n# Calculate confusion matrix\nTP = sum(1 for weight in salmon_weights if weight &gt;= threshold)\nFN = sum(1 for weight in salmon_weights if weight &lt; threshold)\nTN = sum(1 for weight in seabass_weights if weight &lt; threshold)\nFP = sum(1 for weight in seabass_weights if weight &gt;= threshold)\n\nimport matplotlib.patches as mpatches\n\nfig, ax = plt.subplots(figsize=(9, 7))\n\n# Confusion matrix data\ncm_data = np.array([[TP, FP], [FN, TN]])\n\n# Color map: 緑=正解、ピンク=誤り\ncolors = np.array([['#90EE90', '#FFB6C6'], ['#FFB6C6', '#90EE90']])\n\n# Draw cells\nfor i in range(2):\n    for j in range(2):\n        ax.add_patch(mpatches.Rectangle((j, 1-i), 1, 1, \n                                        facecolor=colors[i, j], \n                                        edgecolor='black', linewidth=2.5))\n        ax.text(j + 0.5, 1.5 - i, str(cm_data[i, j]), \n               ha='center', va='center', fontsize=48, fontweight='bold')\n        \n        # Add label\n        if i == 0 and j == 0:\n            label = 'TP'\n        elif i == 0 and j == 1:\n            label = 'FP'\n        elif i == 1 and j == 0:\n            label = 'FN'\n        else:\n            label = 'TN'\n        ax.text(j + 0.5, 1.15 - i, label, \n               ha='center', va='center', fontsize=16, style='italic', color='gray')\n\n# Set labels\nax.set_xlim(0, 2)\nax.set_ylim(0, 2)\nax.set_xticks([0.5, 1.5])\nax.set_yticks([0.5, 1.5])\nax.set_xticklabels(['Actual: Salmon', 'Actual: Sea bass'], fontsize=13, fontweight='bold')\nax.set_yticklabels(['Predict: Sea bass', 'Predict: Salmon'], fontsize=13, fontweight='bold')\nax.set_aspect('equal')\nax.tick_params(length=0)\nax.set_title(f'Confusion Matrix (Threshold = {threshold}g)', \n            fontsize=16, fontweight='bold', pad=20)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "classification-basics-1d.html#判定器の評価",
    "href": "classification-basics-1d.html#判定器の評価",
    "title": "機械学習：分類問題",
    "section": "判定器の評価",
    "text": "判定器の評価\n\n\ncode\n# Calculate metrics\naccuracy = (TP + TN) / (TP + TN + FP + FN)\nprecision = TP / (TP + FP) if (TP + FP) &gt; 0 else 0\nrecall = TP / (TP + FN) if (TP + FN) &gt; 0 else 0\n\nmetrics = pd.DataFrame({\n    '指標': ['Accuracy', 'Precision', 'Recall'],\n    '値': [f'{accuracy:.3f}', f'{precision:.3f}', f'{recall:.3f}'],\n    '意味': [\n        '全体の正解率',\n        'Salmonと判定した中で実際にSalmonの割合',\n        '実際のSalmonをどれだけ検出できたか',\n    ]\n})\n\nfrom IPython.display import HTML\ntable_html = metrics.to_html(index=False, justify='center')\ndisplay(HTML(f\"&lt;div style='font-size:24px'&gt;{table_html}&lt;/div&gt;\"))\n\n\n\n\n\n\n指標\n値\n意味\n\n\n\n\nAccuracy\n0.775\n全体の正解率\n\n\nPrecision\n0.789\nSalmonと判定した中で実際にSalmonの割合\n\n\nRecall\n0.750\n実際のSalmonをどれだけ検出できたか\n\n\n\n\n\n\n\\[\\text{Accuracy} = \\frac{TP + TN}{\\text{全体}} = \\frac{\\text{正しく分類できた数}}{\\text{全データ数}}\\]\n\\[\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{\\text{正しいSalmon判定}}{\\text{Salmonと判定した全て}}\\]\n\\[\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{\\text{検出できたSalmon}}{\\text{実際のSalmon全て}}\\]"
  },
  {
    "objectID": "classification-basics-1d.html#しきい値を変えると",
    "href": "classification-basics-1d.html#しきい値を変えると",
    "title": "機械学習：分類問題",
    "section": "しきい値を変えると？",
    "text": "しきい値を変えると？\n\n\ncode\nthresholds = [1600, 1800, 2000]\nresults = []\n\nfor threshold in thresholds:\n    TP = sum(1 for weight in salmon_weights if weight &gt;= threshold)\n    FN = sum(1 for weight in salmon_weights if weight &lt; threshold)\n    TN = sum(1 for weight in seabass_weights if weight &lt; threshold)\n    FP = sum(1 for weight in seabass_weights if weight &gt;= threshold)\n    \n    accuracy = (TP + TN) / (TP + TN + FP + FN)\n    precision = TP / (TP + FP) if (TP + FP) &gt; 0 else 0\n    recall = TP / (TP + FN) if (TP + FN) &gt; 0 else 0\n    \n    results.append({\n        '境界線': f'{threshold}g',\n        'TP': TP,\n        'FP': FP,\n        'FN': FN,\n        'TN': TN,\n        'Precision': f'{precision:.1%}',\n        'Recall': f'{recall:.1%}',\n        'Accuracy': f'{accuracy:.1%}'\n    })\n\ndf = pd.DataFrame(results)\ndf\n\n\n\n\n\n\n\n\n\n境界線\nTP\nFP\nFN\nTN\nPrecision\nRecall\nAccuracy\n\n\n\n\n0\n1600g\n17\n8\n3\n12\n68.0%\n85.0%\n72.5%\n\n\n1\n1800g\n15\n4\n5\n16\n78.9%\n75.0%\n77.5%\n\n\n2\n2000g\n8\n2\n12\n18\n80.0%\n40.0%\n65.0%\n\n\n\n\n\n\n\n\n\n\n\n\n\n重要\n\n\n\n境界を下げる（1600g） → Recall ↑、Precision ↓\n\nSalmonを逃さない（FN減）が、Sea bassの誤判定（FP増）\n\n境界を上げる（2000g） → Precision ↑、Recall ↓\n\nSalmon判定の信頼性は高いが、Salmonを見逃す（FN増）\n\n\n\n\n\nPrecision と Recall は トレードオフの関係"
  },
  {
    "objectID": "classification-basics-1d.html#しきい値の比較1600g-vs-2000g",
    "href": "classification-basics-1d.html#しきい値の比較1600g-vs-2000g",
    "title": "機械学習：分類問題",
    "section": "しきい値の比較（1600g vs 2000g）",
    "text": "しきい値の比較（1600g vs 2000g）\n\n\n\n\n\n\n\n\n\n\n\nFP=8, FN=3 Precision: 68.0% (低い)\nRecall: 85.0% (高い)\n\n\n\n\n\n\n\n\n\n\nFP=2, FN=12 Precision: 80.0% (高い)\nRecall: 40.0% (低い)"
  },
  {
    "objectID": "classification-basics-1d.html#どの指標を重視すべきか",
    "href": "classification-basics-1d.html#どの指標を重視すべきか",
    "title": "機械学習：分類問題",
    "section": "どの指標を重視すべきか？",
    "text": "どの指標を重視すべきか？\nケース1: 高級レストラン向け出荷\n\nPrecision重視（FPを避ける）\nSea bassの混入を最小化。混入絶対許さないマン\n→ 高めの閾値（2000g）\n\nケース2: 缶詰工場（Salmon専用ライン）\n\nRecall重視（FNを避ける）\nSalmonの取りこぼしを最小化。シャケ全部入れるマン\n→ 低めの閾値（1600g）"
  },
  {
    "objectID": "classification-basics-1d.html#コストを考慮した最適化",
    "href": "classification-basics-1d.html#コストを考慮した最適化",
    "title": "機械学習：分類問題",
    "section": "コストを考慮した最適化",
    "text": "コストを考慮した最適化\n誤分類のコストが異なる場合、総コストを最小化する閾値を選べます。 \\[\n\\begin{aligned}\n&\\text{argmin}(\\text{総コスト}) = N(\\text{FP}) \\times \\text{Cost}(\\text{FP}) + N(\\text{FN}) \\times \\text{Cost}(\\text{FN})\\\\\n&\\quad \\small{N(x): x\\text{の個数},\\; \\text{Cost}(x): x\\text{ 1個あたりのコスト}}\n\\end{aligned}\n\\]\n\n\ncode\n# 例：Sea bass混入のクレーム対応コストが高い場合\nC_FP = 10000  # Sea bass混入のクレーム対応コスト（円/匹）\nC_FN = 500    # Salmon再選別コスト（円/匹）\n\ncost_results = []\nfor threshold in [1600, 1800, 2000]:\n    TP = sum(1 for weight in salmon_weights if weight &gt;= threshold)\n    FN = sum(1 for weight in salmon_weights if weight &lt; threshold)\n    TN = sum(1 for weight in seabass_weights if weight &lt; threshold)\n    FP = sum(1 for weight in seabass_weights if weight &gt;= threshold)\n    \n    total_cost = FP * C_FP + FN * C_FN\n    cost_results.append({\n        '境界線': f'{threshold}g',\n        'FP': FP,\n        'FN': FN,\n        '総コスト': f'¥{total_cost:,}'\n    })\ncost_df = pd.DataFrame(cost_results)\n\nmin_idx = cost_df.index[cost_df['総コスト'].str.replace('¥', '').str.replace(',', '').astype(int).idxmin()]\ncost_df.loc[min_idx, '境界線'] = '★ ' + cost_df.loc[min_idx, '境界線']\ncost_df.style.hide(axis='index')\n\n\n\n\n\n\n\n境界線\nFP\nFN\n総コスト\n\n\n\n\n1600g\n8\n3\n¥81,500\n\n\n1800g\n4\n5\n¥42,500\n\n\n★ 2000g\n2\n12\n¥26,000\n\n\n\n\n\nこのコスト関数での最適なしきい値: 2000g（総コスト最小）"
  },
  {
    "objectID": "classification-basics-1d.html#試行1の結論",
    "href": "classification-basics-1d.html#試行1の結論",
    "title": "機械学習：分類問題",
    "section": "試行1の結論",
    "text": "試行1の結論\n\n\n\n\n\n\n重要なポイント\n\n\n\n重さだけでは完全な分類は不可能\n\nデータの分布が重なっている\n\n境界線は「方針の選択」\n\n正解/不正解ではなく、用途に応じた最適化\n\n評価指標はトレードオフ\n\nPrecision ↑ → Recall ↓\nどちらを重視するかは用途次第"
  },
  {
    "objectID": "classification-basics-1d.html#練習問題",
    "href": "classification-basics-1d.html#練習問題",
    "title": "機械学習：分類問題",
    "section": "練習問題",
    "text": "練習問題\n下記のデータ（それぞれ20匹ずつ追加した）について、以下の0-3までのことを実行して分類器をつくれ。\n\nsalmon_weights = [2149, 1959, 2194, 2457, 1930, 1930, 2474, 2230, 1859, 2163,\n                  1861, 1860, 2073, 1426, 1483, 1831, 1696, 2094, 1728, 1576,\n                  2028, 1903, 2057, 1629, 1884, 1884, 2240, 2081, 1838, 2037,\n                  1839, 1738, 1978, 1754, 1991, 1720, 1731, 1992, 1752, 1652]\nseabass_weights = [1677, 1327, 1486, 1440, 1857, 1476, 1749, 2203, 1979, 1468,\n                   2015, 1528, 1612, 1183, 1436, 1625, 1261, 1701, 1420, 1509,\n                   1387, 1345, 1505, 1204, 1381, 1513, 1259, 1567, 1370, 1432,\n                   1370, 1864, 1488, 1278, 1657, 1245, 1533, 1096, 1223, 1531]"
  },
  {
    "objectID": "classification-basics-1d.html#以下をレポートで提出",
    "href": "classification-basics-1d.html#以下をレポートで提出",
    "title": "機械学習：分類問題",
    "section": "以下をレポートで提出",
    "text": "以下をレポートで提出\n\nまず何をするべきかを考えて実行せよ。\n閾値を1800gにした場合の TP, FP, FN, TN を計算せよ\n1800gの場合の Accuracy, Precision, Recall を求めよ。\n1700gの場合の Accuracy, Precision, Recall を求めよ。\n\\(C_{FP}=5000\\)円、\\(C_{FN}=1000\\)円としたとき、総コストを最小にするしきい値を求めよ"
  },
  {
    "objectID": "classification-basics-1d.html#問0の答え",
    "href": "classification-basics-1d.html#問0の答え",
    "title": "機械学習：分類問題",
    "section": "問0の答え",
    "text": "問0の答え"
  }
]